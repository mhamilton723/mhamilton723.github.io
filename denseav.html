<!DOCTYPE html><!--  This site was created in Webflow. https://www.webflow.com  --><!--  Last Published: Wed Jun 05 2024 22:58:02 GMT+0000 (Coordinated Universal Time)  -->
<html data-wf-page="664684c5f1e5c9f4cd8d0fec" data-wf-site="5fd97fcae7aaf2c02444f619">
<head>
  <meta charset="utf-8">
  <title>DenseAV</title>
  <meta content="Separating the &quot;Chirp&quot; from the &quot;Chat&quot;: Self-supervised Visual Grounding of Sound and Language - CVPR 2024" name="description">
  <meta content="DenseAV" property="og:title">
  <meta content="Separating the &quot;Chirp&quot; from the &quot;Chat&quot;: Self-supervised Visual Grounding of Sound and Language - CVPR 2024" property="og:description">
  <meta content="https://marhamilresearch4.blob.core.windows.net/feature-upsampling-public/teaser_wide.jpg" property="og:image">
  <meta content="DenseAV" property="twitter:title">
  <meta content="Separating the &quot;Chirp&quot; from the &quot;Chat&quot;: Self-supervised Visual Grounding of Sound and Language - CVPR 2024" property="twitter:description">
  <meta content="https://marhamilresearch4.blob.core.windows.net/feature-upsampling-public/teaser_wide.jpg" property="twitter:image">
  <meta property="og:type" content="website">
  <meta content="summary_large_image" name="twitter:card">
  <meta content="width=device-width, initial-scale=1" name="viewport">
  <meta content="Webflow" name="generator">
  <link href="css/normalize.css" rel="stylesheet" type="text/css">
  <link href="css/webflow.css" rel="stylesheet" type="text/css">
  <link href="css/mhamilton723.webflow.css" rel="stylesheet" type="text/css">
  <link href="https://fonts.googleapis.com" rel="preconnect">
  <link href="https://fonts.gstatic.com" rel="preconnect" crossorigin="anonymous">
  <script src="https://ajax.googleapis.com/ajax/libs/webfont/1.6.26/webfont.js" type="text/javascript"></script>
  <script type="text/javascript">WebFont.load({  google: {    families: ["Inconsolata:400,700","Roboto:100,300,regular,500,700"]  }});</script>
  <script type="text/javascript">!function(o,c){var n=c.documentElement,t=" w-mod-";n.className+=t+"js",("ontouchstart"in o||o.DocumentTouch&&c instanceof DocumentTouch)&&(n.className+=t+"touch")}(window,document);</script>
  <link href="images/favicon.ico" rel="shortcut icon" type="image/x-icon">
  <link href="images/webclip.png" rel="apple-touch-icon">
  <script async="" src="https://www.googletagmanager.com/gtag/js?id=G-V1K6GK8BBM"></script>
  <script type="text/javascript">window.dataLayer = window.dataLayer || [];function gtag(){dataLayer.push(arguments);}gtag('js', new Date());gtag('set', 'developer_id.dZGVlNj', true);gtag('config', 'G-V1K6GK8BBM');</script>
</head>
<body class="body">
  <div data-collapse="small" data-animation="over-right" data-duration="400" data-easing="ease" data-easing2="ease" role="banner" class="nav w-nav">
    <div class="nav-container w-container">
      <nav role="navigation" class="nav-menu w-nav-menu">
        <div class="div-block">
          <a href="#examples" class="nav-link w-nav-link">Examples</a>
          <a href="#Video" class="nav-link w-nav-link">Video</a>
          <a href="#Abstract" class="nav-link w-nav-link">Abstract</a>
          <a href="#About" class="nav-link w-nav-link">About</a>
          <a href="https://marhamilresearch4.blob.core.windows.net/feature-upsampling-public/FeatUp_ICLR_2024.pdf" class="nav-link w-nav-link">Paper</a>
          <a href="#Related-Projects" class="nav-link w-nav-link">Related Projects</a>
          <a href="#Contact" class="nav-link w-nav-link">Contact</a>
        </div>
      </nav>
      <div class="menu-button w-nav-button">
        <div class="icon w-icon-nav-menu"></div>
      </div>
    </div>
  </div>
  <div id="Hero" class="paper-hero">
    <div class="hero-wrap"></div>
    <h1 class="paper-title">Separating the &quot;Chirp&quot; from the &quot;Chat&quot;:<br> Self-supervised Visual Grounding<br> of Sound and Language</h1>
    <h1 class="h2">CVPR¬†2024</h1>
    <div class="paper-button-holder">
      <a target="_blank" href="https://aka.ms/denseav-paper" class="button w-button">Paper</a>
      <a target="_blank" href="https://aka.ms/denseav-code" class="button w-button">Code</a>
      <a target="_blank" href="https://huggingface.co/spaces/mhamilton723/FeatUp" class="button w-button">ü§ó Demo</a>
      <a target="_blank" href="https://colab.research.google.com/github/mhamilton723/FeatUp/blob/main/example_usage.ipynb" class="button w-button">Collab Notebook</a>
      <a target="_blank" href="https://aka.ms/denseav-dataset" class="button w-button">Dataset</a>
    </div>
    <div class="spacer"></div>
    <h1 class="paper-names">
      <a href="https://mhamilton.net" target="_blank" class="link">Mark Hamilton</a>, <a href="https://www.robots.ox.ac.uk/~az/" target="_blank" class="link">Andrew Zisserman</a>, <a href="https://research.google/people/john-hershey/" target="_blank" class="link">John R. Hershey,</a>
      <a href="https://billf.mit.edu/about/bio" target="_blank" class="link">William T. Freeman</a>
    </h1>
    <div class="w-layout-grid affiliationholder"><img src="images/mit_logo_white.svg" loading="lazy" alt="" id="w-node-_43a3413d-4adf-1800-f7a1-439b5010f325-cd8d0fec" class="affiliation-image"><img src="images/msft_logo_white.svg" loading="lazy" alt="" class="affiliation-image"><img src="images/Google_2015_logo_colorless_mourning_period.svg" loading="lazy" alt="" class="affiliation-image"><img src="images/University_of_Oxford.svg" loading="lazy" alt="" class="affiliation-image"></div>
    <div class="paper-page-section-width"><img src="images/hero_fig_black.jpg" loading="lazy" width="559" sizes="(max-width: 479px) 89vw, (max-width: 767px) 93vw, (max-width: 991px) 88vw, 750px" alt="" srcset="images/hero_fig_black-p-500.jpg 500w, images/hero_fig_black-p-800.jpg 800w, images/hero_fig_black-p-1080.jpg 1080w, images/hero_fig_black-p-1600.jpg 1600w, images/hero_fig_black-p-2000.jpg 2000w, images/hero_fig_black-p-2600.jpg 2600w, images/hero_fig_black-p-3200.jpg 3200w, images/hero_fig_black.jpg 4685w" class="paper-image">
      <div class="tldr-text"><strong>TL;DR</strong>: Our model, DenseAV, learns the meaning of words and the location of sounds (visual grounding) without supervision or text. </div>
      <h1 id="examples" class="paper-section-header">Examples</h1>
      <h1 class="denseav-example-subheader">(Unmute videos for Audio)</h1>
    </div>
    <div class="paper-page-section-width">
      <h1 class="denseav-example-subheader">DenseAV Discovers Language from Watching Videos:</h1>
      <div class="w-layout-grid denseav-video-grid">
        <div id="w-node-_2e6c9f9f-0131-654e-8ec4-0b2426b39a24-cd8d0fec" class="comparevideoholder">
          <div id="w-node-_2e6c9f9f-0131-654e-8ec4-0b2426b39a2c-cd8d0fec" class="html-embed w-embed">
            <style>
    @media (max-width: 480px) {
        .responsive-video {
            max-height: 200px;
        }
    }
</style>
            <video playsinline="" autoplay="" loop="" preload="" controls="" muted="" class="responsive-video" style="max-width: 100%; object-fit: contain;">
              <source src="https://marhamilresearch4.blob.core.windows.net/denseav-public/teaser_videos/denseav_elephant.mp4" type="video/mp4">
            </video>
          </div>
        </div>
        <div id="w-node-_2e6c9f9f-0131-654e-8ec4-0b2426b39a2d-cd8d0fec" class="comparevideoholder">
          <div id="w-node-_2e6c9f9f-0131-654e-8ec4-0b2426b39a35-cd8d0fec" class="html-embed w-embed">
            <style>
    @media (max-width: 480px) {
        .responsive-video {
            max-height: 200px;
        }
    }
</style>
            <video playsinline="" autoplay="" loop="" preload="" controls="" muted="" class="responsive-video" style="max-width: 100%; object-fit: contain;">
              <source src="https://marhamilresearch4.blob.core.windows.net/denseav-public/teaser_videos/denseav_peppers.mp4" type="video/mp4">
            </video>
          </div>
        </div>
        <div id="w-node-_2e6c9f9f-0131-654e-8ec4-0b2426b39a36-cd8d0fec" class="comparevideoholder">
          <div id="w-node-_2e6c9f9f-0131-654e-8ec4-0b2426b39a3e-cd8d0fec" class="html-embed w-embed">
            <style>
    @media (max-width: 480px) {
        .responsive-video {
            max-height: 200px;
        }
    }
</style>
            <video playsinline="" autoplay="" loop="" preload="" controls="" muted="" class="responsive-video" style="max-width: 100%; object-fit: contain;">
              <source src="https://marhamilresearch4.blob.core.windows.net/denseav-public/teaser_videos/denseav_viaduct.mp4" type="video/mp4">
            </video>
          </div>
        </div>
      </div>
      <h1 class="denseav-example-subheader">DenseAV Localizes Sound without Supervision:</h1>
      <div class="w-layout-grid denseav-video-grid">
        <div id="w-node-d8aa74ef-823f-7be0-578b-b2d7d1779794-cd8d0fec" class="comparevideoholder">
          <div id="w-node-d8aa74ef-823f-7be0-578b-b2d7d1779795-cd8d0fec" class="html-embed w-embed">
            <style>
    @media (max-width: 480px) {
        .responsive-video {
            max-height: 200px;
        }
    }
</style>
            <video playsinline="" autoplay="" loop="" preload="" controls="" muted="" class="responsive-video" style="max-width: 100%; object-fit: contain;">
              <source src="https://marhamilresearch4.blob.core.windows.net/denseav-public/teaser_videos/denseav_sax.mp4" type="video/mp4">
            </video>
          </div>
        </div>
        <div id="w-node-d8aa74ef-823f-7be0-578b-b2d7d1779796-cd8d0fec" class="comparevideoholder">
          <div id="w-node-d8aa74ef-823f-7be0-578b-b2d7d1779797-cd8d0fec" class="html-embed w-embed">
            <style>
    @media (max-width: 480px) {
        .responsive-video {
            max-height: 200px;
        }
    }
</style>
            <video playsinline="" autoplay="" loop="" preload="" controls="" muted="" class="responsive-video" style="max-width: 100%; object-fit: contain;">
              <source src="https://marhamilresearch4.blob.core.windows.net/denseav-public/teaser_videos/denseav_train.mp4" type="video/mp4">
            </video>
          </div>
        </div>
        <div id="w-node-d8aa74ef-823f-7be0-578b-b2d7d1779798-cd8d0fec" class="comparevideoholder">
          <div id="w-node-d8aa74ef-823f-7be0-578b-b2d7d1779799-cd8d0fec" class="html-embed w-embed">
            <style>
    @media (max-width: 480px) {
        .responsive-video {
            max-height: 200px;
        }
    }
</style>
            <video playsinline="" autoplay="" loop="" preload="" controls="" muted="" class="responsive-video" style="max-width: 100%; object-fit: contain;">
              <source src="https://marhamilresearch4.blob.core.windows.net/denseav-public/teaser_videos/denseav_bird.mp4" type="video/mp4">
            </video>
          </div>
        </div>
      </div>
      <h1 class="denseav-example-subheader">Unsupervised Disentanglemnt of Sound and Language:</h1>
      <div class="w-layout-grid denseav-video-grid-2x2">
        <div id="w-node-_09f69598-516e-a7c5-6d38-013b7eb36b11-cd8d0fec" class="comparevideoholder">
          <div id="w-node-_09f69598-516e-a7c5-6d38-013b7eb36b12-cd8d0fec" class="html-embed w-embed">
            <style>
    @media (max-width: 480px) {
        .responsive-video {
            max-height: 200px;
        }
    }
</style>
            <video playsinline="" autoplay="" loop="" preload="" controls="" muted="" class="responsive-video" style="max-width: 100%; object-fit: contain;">
              <source src="https://marhamilresearch4.blob.core.windows.net/denseav-public/teaser_videos/denseav_puppies.mp4" type="video/mp4">
            </video>
          </div>
        </div>
        <div id="w-node-_09f69598-516e-a7c5-6d38-013b7eb36b13-cd8d0fec" class="comparevideoholder">
          <div id="w-node-_09f69598-516e-a7c5-6d38-013b7eb36b14-cd8d0fec" class="html-embed w-embed">
            <style>
    @media (max-width: 480px) {
        .responsive-video {
            max-height: 200px;
        }
    }
</style>
            <video playsinline="" autoplay="" loop="" preload="" controls="" muted="" class="responsive-video" style="max-width: 100%; object-fit: contain;">
              <source src="https://marhamilresearch4.blob.core.windows.net/denseav-public/teaser_videos/denseav_sofa.mp4" type="video/mp4">
            </video>
          </div>
        </div>
        <div id="w-node-_09f69598-516e-a7c5-6d38-013b7eb36b15-cd8d0fec" class="comparevideoholder">
          <div id="w-node-_09f69598-516e-a7c5-6d38-013b7eb36b16-cd8d0fec" class="html-embed w-embed">
            <style>
    @media (max-width: 480px) {
        .responsive-video {
            max-height: 200px;
        }
    }
</style>
            <video playsinline="" autoplay="" loop="" preload="" controls="" muted="" class="responsive-video" style="max-width: 100%; object-fit: contain;">
              <source src="https://marhamilresearch4.blob.core.windows.net/denseav-public/teaser_videos/denseav_racecar.mp4" type="video/mp4">
            </video>
          </div>
        </div>
        <div id="w-node-b75a344e-16e0-d047-54bb-1bda891c1714-cd8d0fec" class="comparevideoholder">
          <div id="w-node-b75a344e-16e0-d047-54bb-1bda891c1715-cd8d0fec" class="html-embed w-embed">
            <style>
    @media (max-width: 480px) {
        .responsive-video {
            max-height: 200px;
        }
    }
</style>
            <video playsinline="" autoplay="" loop="" preload="" controls="" muted="" class="responsive-video" style="max-width: 100%; object-fit: contain;">
              <source src="https://marhamilresearch4.blob.core.windows.net/denseav-public/teaser_videos/denseav_computer.mp4" type="video/mp4">
            </video>
          </div>
        </div>
      </div>
    </div>
  </div>
  <div id="Video" class="section">
    <div class="section-margin">
      <div class="paper-page-section-width">
        <div style="padding-top:56.17021276595745%" class="w-embed-youtubevideo youtube"><iframe src="https://www.youtube.com/embed/GKEi5djS_nM?rel=0&amp;controls=1&amp;autoplay=0&amp;mute=0&amp;start=0" frameborder="0" style="position:absolute;left:0;top:0;width:100%;height:100%;pointer-events:auto" allow="autoplay; encrypted-media" allowfullscreen="" title="AI Learns Language from Scratch"></iframe></div>
      </div>
    </div>
  </div>
  <div id="Abstract" class="section">
    <div class="section-margin">
      <div class="paper-page-section-width">
        <h1 class="paper-section-header">Abstract</h1>
        <div class="small-spacer"></div>
        <h1 id="Abstract" class="paper-page-text-block">We present DenseAV, a novel dual encoder grounding architecture that learns high-resolution, semantically meaningful, and audio-visually aligned features solely through watching videos. We show that DenseAV can discover the ``meaning&#x27;&#x27; of words and the ``location&#x27;&#x27; of sounds without explicit localization supervision. Furthermore, it automatically discovers and distinguishes between these two types of associations without supervision. We show that DenseAV&#x27;s localization abilities arise from a new multi-head feature aggregation operator that directly compares dense image and audio representations for contrastive learning. In contrast, many other systems that learn ``global&#x27;&#x27; audio and video representations cannot localize words and sound. Finally, we contribute two new datasets to improve the evaluation of AV representations through speech and sound prompted semantic segmentation. On these and other datasets we show DenseAV dramatically outperforms the prior art on speech and sound prompted semantic segmentation. DenseAV outperforms the previous state-of-the-art, ImageBind, on cross-modal retrieval using fewer than half of the parameters.</h1>
      </div>
    </div>
  </div>
  <div id="About" class="section">
    <div class="section-margin">
      <div class="paper-page-section-width">
        <h1 class="paper-section-header">Audio-Video Contrastive Learning</h1>
        <div class="small-spacer"></div>
        <h1 class="paper-page-text-block">DenseAV can learn the meaning of words and the location of sounds using only self-supervision from video. To learn these patterns, DenseAV uses audio-video contrastiv learning to associate sound with the visual world. Intuitively speaking, its much easier to predict what you are seeing from what you are hearing when you understand language and can recognize sounds. This is how DenseAV can learn without labels.</h1><img src="images/contrastive_learning.jpg" loading="lazy" sizes="(max-width: 479px) 93vw, (max-width: 767px) 95vw, (max-width: 991px) 90vw, 750px" srcset="images/contrastive_learning-p-500.jpg 500w, images/contrastive_learning-p-800.jpg 800w, images/contrastive_learning-p-1080.jpg 1080w, images/contrastive_learning-p-1600.jpg 1600w, images/contrastive_learning-p-2000.jpg 2000w, images/contrastive_learning-p-2600.jpg 2600w, images/contrastive_learning.jpg 2829w" alt="" class="paper-image">
      </div>
      <div class="paper-page-section-width">
        <h1 class="paper-section-header">Most Contrastive Learners Cannot Localize Sound or Language</h1>
        <div class="small-spacer"></div><img src="images/failures.jpg" loading="lazy" width="1000" sizes="(max-width: 479px) 93vw, (max-width: 767px) 95vw, (max-width: 991px) 90vw, 750px" alt="" srcset="images/failures-p-500.jpg 500w, images/failures-p-800.jpg 800w, images/failures-p-1080.jpg 1080w, images/failures-p-1600.jpg 1600w, images/failures-p-2000.jpg 2000w, images/failures-p-2600.jpg 2600w, images/failures-p-3200.jpg 3200w, images/failures.jpg 4243w" class="paper-image">
        <h1 class="paper-page-text-block">Interestingly, contrastive learning with CLS¬†tokens or average pooled representations isnt enough to be able to localize objects from sound and language. DenseAV uses a contrastive similarity based on inner products between local audio and visual representation tokens. This dramatically improves its ability to localize information.<br>‚Äç</h1><img src="images/feature-inner-products.jpg" loading="lazy" sizes="(max-width: 479px) 93vw, (max-width: 767px) 95vw, (max-width: 991px) 90vw, 750px" srcset="images/feature-inner-products-p-500.jpg 500w, images/feature-inner-products-p-800.jpg 800w, images/feature-inner-products-p-1080.jpg 1080w, images/feature-inner-products-p-1600.jpg 1600w, images/feature-inner-products-p-2000.jpg 2000w, images/feature-inner-products-p-2600.jpg 2600w, images/feature-inner-products-p-3200.jpg 3200w, images/feature-inner-products.jpg 3334w" alt="" class="paper-image">
      </div>
      <div class="paper-page-section-width">
        <h1 class="paper-section-header">Unsupervised Disentanglement of Sound and Language</h1>
        <div class="small-spacer"></div><img src="images/2_head_vis.jpg" loading="lazy" width="2456.5" sizes="(max-width: 479px) 93vw, (max-width: 767px) 95vw, (max-width: 991px) 90vw, 750px" alt="" srcset="images/2_head_vis-p-500.jpg 500w, images/2_head_vis-p-800.jpg 800w, images/2_head_vis-p-1080.jpg 1080w, images/2_head_vis-p-1600.jpg 1600w, images/2_head_vis-p-2000.jpg 2000w, images/2_head_vis-p-2600.jpg 2600w, images/2_head_vis-p-3200.jpg 3200w, images/2_head_vis.jpg 3669w" class="paper-image">
        <h1 class="paper-page-text-block">Theres many ways that a sound can be related to an visual object. For instance, the word &quot;dog&quot; and the sound of a bark both conjure the image of a dog despite being very different types of sound. In an analogy with multi-head attention we provide DenseAV with multiple features to compute inner products with. Amazingly, DenseAV naturally organizes it&#x27;s features into sound-features and language features without knowing a-priori what is sound and what is language.</h1><img src="images/2_heads.jpg" loading="lazy" width="2456.5" sizes="(max-width: 479px) 93vw, (max-width: 767px) 95vw, (max-width: 991px) 90vw, 750px" alt="" srcset="images/2_heads-p-500.jpg 500w, images/2_heads-p-800.jpg 800w, images/2_heads-p-1080.jpg 1080w, images/2_heads-p-1600.jpg 1600w, images/2_heads-p-2000.jpg 2000w, images/2_heads-p-2600.jpg 2600w, images/2_heads-p-3200.jpg 3200w, images/2_heads.jpg 4384w" class="paper-image">
      </div>
    </div>
  </div>
  <div id="Paper" class="section">
    <div class="section-margin">
      <div class="paper-page-section-width">
        <h1 class="paper-section-header">Paper</h1>
        <div class="small-spacer"></div>
        <a href="https://aka.ms/denseav-paper" target="_blank" class="w-inline-block"><img src="images/paper-view.jpg" loading="lazy" sizes="(max-width: 479px) 93vw, (max-width: 767px) 95vw, (max-width: 991px) 90vw, 750px" srcset="images/paper-view-p-500.jpg 500w, images/paper-view-p-800.jpg 800w, images/paper-view-p-1080.jpg 1080w, images/paper-view-p-1600.jpg 1600w, images/paper-view-p-2000.jpg 2000w, images/paper-view-p-2600.jpg 2600w, images/paper-view-p-3200.jpg 3200w, images/paper-view.jpg 3436w" alt="" class="paper-thumbnail"></a>
      </div>
      <div class="paper-page-section-width">
        <h1 class="paper-section-header">Bibtex</h1>
        <div class="small-spacer"></div>
        <div class="code-block"><code>@article{hamilton2024separating,<br>¬†¬†¬†¬†title={Separating the &quot;Chirp&quot; from the &quot;Chat&quot;: Self-supervised Visual Grounding of Sound and Language},<br>¬†¬†¬†¬†author={Hamilton, Mark and Zisserman, Andrew and Hershey, John and Freeman, William},<br>¬†¬†¬†¬†journal={TODO},<br>¬†¬†¬†¬†year={2024}<br>}</code></div>
      </div>
    </div>
  </div>
  <div id="Related-Projects" class="section">
    <div class="section-margin">
      <div class="paper-page-section-width">
        <h1 class="paper-section-header">Related Projects</h1>
        <div class="small-spacer"></div>
        <div class="paper-entry">
          <div class="pub-image-div">
            <div class="pub-img w-embed">
              <style>
    @media (max-width: 480px) {
        .responsive-video {
            max-height: 200px;
        }
    }
</style>
              <video playsinline="" autoplay="" loop="" preload="" muted="" class="responsive-video" style="max-width: 100%; object-fit: contain;">
                <source src="https://mhamilton.net/videos/featup_teaser.mp4" type="video/mp4">
              </video>
            </div>
          </div>
          <div class="pub-item">
            <h2 class="paper-titile">FeatUp: A Model-Agnostic Frameworkfor Features at Any Resolution<br></h2>
            <div class="paper-buttons">
              <a target="_blank" href="featup.html" class="button w-button">Website</a>
              <a target="_blank" href="https://aka.ms/featup-paper" class="button w-button">Paper</a>
              <a target="_blank" href="https://github.com/mhamilton723/FeatUp" class="button w-button">Github</a>
              <a target="_blank" href="https://raw.githubusercontent.com/mhamilton723/mhamilton723.github.io/master/files/bib/fu2024featup.bib" class="button w-button">BibTex</a>
            </div>
            <h1 class="author-list">We improve the spatial resolution of any vision backbone by 16-32x without changing their features&#x27; semantics.</h1>
          </div>
        </div>
        <div class="paper-entry">
          <div class="pub-image-div"><img src="images/ezgif.com-gif-maker-3.gif" width="436" alt="" loading="lazy" class="pub-img"></div>
          <div class="pub-item">
            <h2 class="paper-titile">Unsupervised Semantic Segmentation by Distilling Feature Correspondences<br></h2>
            <div class="paper-buttons">
              <a target="_blank" href="stego.html" class="button w-button">Website</a>
              <a target="_blank" href="https://arxiv.org/abs/2203.08414" class="button w-button">Paper</a>
              <a target="_blank" href="https://iclr.cc/virtual/2022/poster/6068" class="button w-button">Talk</a>
              <a target="_blank" href="https://github.com/mhamilton723/STEGO" class="button w-button">Github</a>
              <a target="_blank" href="https://raw.githubusercontent.com/mhamilton723/mhamilton723.github.io/master/files/bib/hamilton2022unsupervised.bib" class="button w-button">BibTex</a>
            </div>
            <h1 class="author-list">We show that inner products between deep features hold a key to solving unsupervised semantic segmentation. In particular we distill these features into high quality unsupervised semantic segmentaions.</h1>
          </div>
        </div>
        <div class="paper-entry last-entry">
          <div class="pub-image-div">
            <div class="pub-img w-embed"><video width="100%" playsinline="" autoplay="" loop="" preload="" muted="">
                <source src="https://mhamilton.net/videos/projection.mp4" type="video/mp4">
              </video></div>
          </div>
          <div class="pub-item">
            <h2 class="paper-titile">Axiomatic Explanations for Visual Search, Retrieval, and Similarity Learning<br></h2>
            <div class="paper-buttons">
              <a target="_blank" href="#" class="button w-button">Website</a>
              <a target="_blank" href="https://arxiv.org/abs/2103.00370" class="button w-button">Paper</a>
              <a target="_blank" href="https://iclr.cc/virtual/2022/poster/6983" class="button w-button">Talk</a>
              <a target="_blank" href="https://raw.githubusercontent.com/mhamilton723/mhamilton723.github.io/master/files/bib/hamilton2021model.bib" class="button w-button">BibTex</a>
            </div>
            <h1 class="author-list">We show that inner products between deep vision features can be interpreted as a generalization of Shapley Values for contrastive image similarity networks. We explore this theory and show that it provides a unique axiomatic characterization of contrastive model explanation methods.</h1>
          </div>
        </div>
      </div>
    </div>
  </div>
  <div id="Contact" class="section">
    <div class="section-margin">
      <div class="paper-page-section-width">
        <h1 class="paper-section-header">Contact</h1>
        <div class="small-spacer"></div>
        <h1 class="paper-page-text-block">For feedback, questions, or press inquiries please contact <a href="mailto:markth@mit.edu?subject=FeatUp" class="link">Mark Hamilton</a>
        </h1>
      </div>
      <div class="footer"></div>
    </div>
  </div>
  <script src="https://d3e54v103j8qbb.cloudfront.net/js/jquery-3.5.1.min.dc5e7f18c8.js?site=5fd97fcae7aaf2c02444f619" type="text/javascript" integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin="anonymous"></script>
  <script src="js/webflow.js" type="text/javascript"></script><!-- <script async src="lib/particles.min.js"></script>  -->
</body>
</html>