<!DOCTYPE html><!--  This site was created in Webflow. http://www.webflow.com  -->
<!--  Last Published: Wed Jun 08 2022 21:00:42 GMT+0000 (Coordinated Universal Time)  -->
<html data-wf-page="623b9ad4c39a2570c93a65c4" data-wf-site="5fd97fcae7aaf2c02444f619">
<head>
  <meta charset="utf-8">
  <title>STEGO: Unsupervised Semantic Segmentation by Distilling Feature Correspondences</title>
  <meta content="State of the art unsupervised semantic segmentation." name="description">
  <meta content="STEGO: Unsupervised Semantic Segmentation by Distilling Feature Correspondences" property="og:title">
  <meta content="State of the art unsupervised semantic segmentation." property="og:description">
  <meta content="https://marhamilresearch4.blob.core.windows.net/stego-public/graphics/stego_overview.jpg" property="og:image">
  <meta content="STEGO: Unsupervised Semantic Segmentation by Distilling Feature Correspondences" property="twitter:title">
  <meta content="State of the art unsupervised semantic segmentation." property="twitter:description">
  <meta content="https://marhamilresearch4.blob.core.windows.net/stego-public/graphics/stego_overview.jpg" property="twitter:image">
  <meta property="og:type" content="website">
  <meta content="summary_large_image" name="twitter:card">
  <meta content="width=device-width, initial-scale=1" name="viewport">
  <meta content="Webflow" name="generator">
  <link href="css/normalize.css" rel="stylesheet" type="text/css">
  <link href="css/webflow.css" rel="stylesheet" type="text/css">
  <link href="css/mhamilton723.webflow.css" rel="stylesheet" type="text/css">
  <script src="https://ajax.googleapis.com/ajax/libs/webfont/1.6.26/webfont.js" type="text/javascript"></script>
  <script type="text/javascript">WebFont.load({  google: {    families: ["Inconsolata:400,700","Roboto:100,300,regular,500,700"]  }});</script>
  <!-- [if lt IE 9]><script src="https://cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv.min.js" type="text/javascript"></script><![endif] -->
  <script type="text/javascript">!function(o,c){var n=c.documentElement,t=" w-mod-";n.className+=t+"js",("ontouchstart"in o||o.DocumentTouch&&c instanceof DocumentTouch)&&(n.className+=t+"touch")}(window,document);</script>
  <link href="images/favicon.ico" rel="shortcut icon" type="image/x-icon">
  <link href="images/webclip.png" rel="apple-touch-icon">
  <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-128882874-1"></script>
  <script type="text/javascript">window.dataLayer = window.dataLayer || [];function gtag(){dataLayer.push(arguments);}gtag('js', new Date());gtag('config', 'UA-128882874-1', {'anonymize_ip': false});</script>
</head>
<body class="body">
  <div data-collapse="small" data-animation="over-right" data-duration="400" data-easing="ease" data-easing2="ease" role="banner" class="nav w-nav">
    <div class="nav-container w-container">
      <nav role="navigation" class="nav-menu w-nav-menu">
        <div class="div-block">
          <a href="#Video" class="nav-link w-nav-link">Video</a>
          <a href="#Abstract" class="nav-link w-nav-link">Abstract</a>
          <a href="#About" class="nav-link w-nav-link">About</a>
          <a href="#Paper" class="nav-link w-nav-link">Paper</a>
          <a href="#Related-Projects" class="nav-link w-nav-link">Related Projects</a>
          <a href="#Contact" class="nav-link w-nav-link">Contact</a>
        </div>
      </nav>
      <div class="menu-button w-nav-button">
        <div class="icon w-icon-nav-menu"></div>
      </div>
    </div>
  </div>
  <div id="Hero" class="paper-hero">
    <div class="hero-wrap"></div>
    <h1 class="paper-title">Unsupervised Semantic Segmentation<br>by Distilling Feature Correspondences</h1>
    <h1 class="h2">ICLR 2022</h1>
    <div class="paper-button-holder">
      <a target="_blank" href="https://arxiv.org/abs/2203.08414" class="button w-button">Paper</a>
      <a target="_blank" href="https://aka.ms/stego-code" class="button w-button">Code</a>
      <a target="_blank" href="https://iclr.cc/virtual/2022/poster/6068" class="button w-button">Talk</a>
    </div>
    <div class="spacer"></div>
    <h1 class="paper-names">
      <a href="https://mhamilton.net" target="_blank" class="link">Mark Hamilton</a>, <a href="https://ztzhang.info/" target="_blank" class="link">Zhoutong Zhang</a>, <a href="http://home.bharathh.info/" target="_blank" class="link">Bharath Hariharan</a>, <a href="https://www.cs.cornell.edu/~snavely/" target="_blank" class="link">Noah Snavely</a>, <a href="https://billf.mit.edu/about/bio" target="_blank" class="link">William T. Freeman</a>
    </h1>
    <div class="w-layout-grid affiliation-grid"><img src="images/mit_logo_white.svg" loading="lazy" id="w-node-_185912c3-010d-fb94-8b11-ceb325f173f4-c93a65c4" alt="" class="affiliation-image"><img src="images/msft_logo_white.svg" loading="lazy" alt="" class="affiliation-image"><img src="images/cornell_logo_simple.svg" loading="lazy" alt="" class="affiliation-image"><img src="images/Google_2015_logo_colorless_mourning_period.svg" loading="lazy" alt="" class="affiliation-image"></div>
  </div>
  <div id="Video" class="section wf-section">
    <div class="section-margin">
      <div class="paper-page-section-width">
        <div style="padding-top:56.17021276595745%" class="w-embed-youtubevideo youtube"><iframe src="https://www.youtube.com/embed/NPub4E4o8BA?rel=0&amp;controls=1&amp;autoplay=0&amp;mute=0&amp;start=0" frameborder="0" style="position:absolute;left:0;top:0;width:100%;height:100%;pointer-events:auto" allow="autoplay; encrypted-media" allowfullscreen="" title="STEGO: Unsupervised Semantic Segmentation by Distilling Feature Correspondences"></iframe></div>
      </div>
    </div>
  </div>
  <div id="Abstract" class="section wf-section">
    <div class="section-margin">
      <div class="paper-page-section-width">
        <div class="small-spacer"></div>
        <h1 class="paper-page-text-block">Unsupervised semantic segmentation aims to discover and localize semantically meaningful categories within image corpora without any form of annotation. To solve this task, algorithms must produce features for every pixel that are both semantically meaningful and compact enough to form distinct clusters. Unlike previous works which achieve this with a single end-to-end framework, we propose to separate feature learning from cluster compactification. Empirically, we show that current unsupervised feature learning frameworks already generate dense features whose correlations are semantically consistent. This observation motivates us to design STEGO (<strong>S</strong>elf-supervised <strong>T</strong>ransformer with <strong>E</strong>nergy-based <strong>G</strong>raph <strong>O</strong>ptimization), a novel framework that distills unsupervised features into high-quality discrete semantic labels. At the core of STEGO is a novel contrastive loss function that encourages features to form compact clusters while preserving their relationships across the corpora. STEGO yields a significant improvement over the prior state of the art, on both the CocoStuff (<strong>+14 mIoU</strong>) and Cityscapes (<strong>+9 mIoU</strong>) semantic segmentation challenges.</h1>
      </div>
    </div>
  </div>
  <div id="About" class="section wf-section">
    <div class="section-margin">
      <div class="paper-page-section-width">
        <h1 class="paper-section-header">Unsupervised semantic segmentation</h1>
        <div class="small-spacer"></div>
        <h1 class="paper-page-text-block">Real-world images can be cluttered with multiple objects making classification feel arbitrary. Furthermore, objects in the real world don&#x27;t always fit in bounding boxes. Semantic segmentation methods aim to avoid these challenges by assigning each pixel of an image its own class label. Conventional semantic segmentation methods are notoriously difficult to train due to their dependence on densely labeled images, which can take <strong>100x longer to create than bounding boxes or class annotations</strong>. This makes it hard to gather sizable and diverse datasets in domains where humans don&#x27;t know the structure a-priori. We sidestep these challenges by learning an<strong> ontology of objects with pixel-level semantic segmentation </strong>through <strong>only self-supervision</strong>. </h1><img src="images/overview.jpg" loading="lazy" sizes="(max-width: 479px) 93vw, (max-width: 767px) 95vw, (max-width: 991px) 90vw, 50vw" srcset="images/overview-p-500.jpeg 500w, images/overview-p-800.jpeg 800w, images/overview-p-1080.jpeg 1080w, images/overview-p-1600.jpeg 1600w, images/overview.jpg 2000w" alt="" class="image-2">
      </div>
      <div class="paper-page-section-width">
        <h1 class="paper-section-header">Deep features connect objects across images</h1>
        <div class="small-spacer"></div>
        <h1 class="paper-page-text-block">Self-supervised contrastive learning enables algorithms to learn intelligent representations for images without supervision. STEGO builds on this work by showing that representations from self-supervised visual transformers like  <a href="https://arxiv.org/abs/2104.14294" id="Link" target="_blank" class="link">Caron et. al.’s DINO</a> are <strong>already</strong> aware of the relationships between objects. By computing the cosine similarity between image features, we can see that similar semantic regions such as grass, motorcycles, and sky are “linked” together by feature similarity. We also show these connections are a particular case of a <a href="https://mhamilton.net/axiomatic.html" target="_blank" class="link">broader theory connecting game theory, economics, and model-explainability</a>.</h1><img src="images/Picture3.gif" loading="lazy" alt="" class="image-3">
      </div>
      <div class="paper-page-section-width">
        <h1 class="paper-section-header">STEGO</h1>
        <div class="small-spacer"></div>
        <h1 class="paper-page-text-block">The STEGO unsupervised segmentation system <strong>learns by distilling correspondences</strong> between images into a set of class labels using a contrastive loss. In particular we aim to learn a segmentation that respects the induced correspondences between objects. To achieve this we train a <strong>shallow segmentation network</strong> on top of the DINO ViT backbone with three contrastive terms that distill connections between an image and itself, similar images, and random other images respectively. If two regions are strongly coupled by deep features we encourage them to share the same class. </h1><img src="images/stego_white.svg" loading="lazy" alt="" class="image-4">
      </div>
      <div class="paper-page-section-width">
        <h1 class="paper-section-header">Results</h1>
        <div class="small-spacer"></div>
        <h1 class="paper-page-text-block">We evaluate the STEGO algorithm on the CocoStuff, Cityscapes, and Potsdam semantic segmentation datasets. Because these methods see no labels, we use a Hungarian matching algorithm to find the best mapping between clusters and dataset classes. We find that STEGO is capable of segmenting complex and cluttered scenes with much higher spatial resolution and sensitivity than the prior art, <a href="https://sites.google.com/view/picie-cvpr2021/home" target="_blank" class="link">PiCIE</a>. This not only yields a substantial qualitative improvement, but also more than <strong>doubles the mean intersection over union (mIoU)</strong>. For results on Cityscapes, and Potsdam see <a href="#Paper" class="link">our paper</a>.</h1><img src="images/Picture6.jpg" loading="lazy" sizes="(max-width: 479px) 85vw, (max-width: 767px) 90vw, (max-width: 991px) 86vw, 50vw" srcset="images/Picture6-p-500.jpeg 500w, images/Picture6-p-1080.jpeg 1080w, images/Picture6-p-1600.jpeg 1600w, images/Picture6.jpg 2000w" alt="" class="image-4">
      </div>
    </div>
  </div>
  <div id="Paper" class="section wf-section">
    <div class="section-margin">
      <div class="paper-page-section-width">
        <h1 class="paper-section-header">Paper</h1>
        <div class="small-spacer"></div>
        <a href="https://arxiv.org/abs/2203.08414" target="_blank" class="w-inline-block"><img src="images/stego_paper.jpg" loading="lazy" sizes="100vw" srcset="images/stego_paper-p-500.jpeg 500w, images/stego_paper-p-800.jpeg 800w, images/stego_paper-p-1080.jpeg 1080w, images/stego_paper-p-1600.jpeg 1600w, images/stego_paper-p-2000.jpeg 2000w, images/stego_paper-p-2600.jpeg 2600w, images/stego_paper-p-3200.jpeg 3200w, images/stego_paper.jpg 4000w" alt="" class="paper-thumbnail"></a>
      </div>
      <div class="paper-page-section-width">
        <h1 class="paper-section-header">Bibtex</h1>
        <div class="small-spacer"></div>
        <div class="code-block">@inproceedings{hamilton2021unsupervised,<br>  title={Unsupervised Semantic Segmentation by Distilling Feature Correspondences},<br>  author={Hamilton, Mark and Zhang, Zhoutong and Hariharan, Bharath and Snavely, Noah and Freeman, William T},<br>  booktitle={International Conference on Learning Representations},<br>  year={2021}<br>}</div>
      </div>
    </div>
  </div>
  <div id="Related-Projects" class="section wf-section">
    <div class="section-margin">
      <div class="paper-page-section-width">
        <h1 class="paper-section-header">Related Projects</h1>
        <div class="small-spacer"></div>
        <div class="paper-entry-2">
          <div class="pub-image-div">
            <div class="pub-img w-embed"><video width="100%" playsinline="" autoplay="" loop="" preload="" muted="">
                <source src="https://mhamilton.net/videos/projection.mp4" type="video/mp4">
              </video></div>
          </div>
          <div class="pub-item">
            <h2 class="paper-titile">Axiomatic Explanations for Visual Search, Retrieval, and Similarity Learning<br></h2>
            <div class="paper-buttons">
              <a target="_blank" href="axiomatic.html" class="button w-button">Website</a>
              <a target="_blank" href="https://arxiv.org/abs/2103.00370" class="button w-button">Paper</a>
              <a target="_blank" href="https://iclr.cc/virtual/2022/poster/6983" class="button w-button">Talk</a>
              <a target="_blank" href="https://raw.githubusercontent.com/mhamilton723/mhamilton723.github.io/master/files/bib/hamilton2021model.bib" class="button w-button">BibTex</a>
            </div>
            <h1 class="author-list">The feature correspondences of STEGO arise from a generalization of Shapley Values for contrastive image similarity networks. We explore this theory and show that it provides a unique axiomatic characterization of contrastive model explanation methods.</h1>
          </div>
        </div>
        <div class="paper-entry-2">
          <div class="pub-image-div">
            <div class="pub-img w-embed"><video width="100%" playsinline="" autoplay="" loop="" preload="" muted="">
                <source src="https://mhamilton.net/videos/CIR.mp4" type="video/mp4">
              </video></div>
          </div>
          <div class="pub-item">
            <h2 class="paper-titile">MosAIc: Finding Artistic Connections across Culture with Conditional Image Retrieval</h2>
            <div class="paper-buttons">
              <a target="_blank" href="https://arxiv.org/abs/2007.07177" class="button w-button">Paper</a>
              <a target="_blank" href="http://www.aka.ms/mosaic" class="button w-button">Website</a>
              <a target="_blank" href="https://note.microsoft.com/MSR-Webinar-Visual-Analogies-Registration-On-Demand.html" class="button w-button">Webinar</a>
              <a target="_blank" href="https://youtu.be/y0iYKr9ayfE" class="button w-button">Talk</a>
              <a target="_blank" href="https://github.com/microsoft/art" class="button w-button">Code</a>
              <a target="_blank" href="https://raw.githubusercontent.com/mhamilton723/mhamilton723.github.io/master/files/bib/hamilton2020conditional.bib" class="button w-button">BibTex</a>
            </div>
            <h1 class="author-list">We introduce a new K-Nearest Neighbor data-structure to enable fast computation of an images conditional nearest neighbors in deep feature space. We show the approach can find &quot;hidden connections&quot; in the visual arts, as well as &quot;blind-spots&quot; in trained Generative Adversarial Networks.</h1>
          </div>
        </div>
      </div>
    </div>
  </div>
  <div id="Related-Projects" class="section wf-section">
    <div id="Contact" class="section-margin">
      <div class="paper-page-section-width">
        <h1 class="paper-section-header">Contact</h1>
        <div class="small-spacer"></div>
        <h1 class="paper-page-text-block">For feedback, questions, or press inquiries please contact <a href="mailto:markth@mit.edu?subject=STEGO" class="link">Mark Hamilton</a>
        </h1>
      </div>
      <div class="footer"></div>
    </div>
  </div>
  <script src="https://d3e54v103j8qbb.cloudfront.net/js/jquery-3.5.1.min.dc5e7f18c8.js?site=5fd97fcae7aaf2c02444f619" type="text/javascript" integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin="anonymous"></script>
  <script src="js/webflow.js" type="text/javascript"></script>
  <!-- [if lte IE 9]><script src="https://cdnjs.cloudflare.com/ajax/libs/placeholders/3.0.2/placeholders.min.js"></script><![endif] -->
  <script async="" src="lib/particles.min.js"></script>
</body>
</html>