<!DOCTYPE html><!--  This site was created in Webflow. https://www.webflow.com  -->
<!--  Last Published: Mon Mar 18 2024 02:24:21 GMT+0000 (Coordinated Universal Time)  -->
<html data-wf-page="65d833482153e0f2f4f8ffe1" data-wf-site="5fd97fcae7aaf2c02444f619">
<head>
  <meta charset="utf-8">
  <title>FeatUp</title>
  <meta content="A Model-Agnostic Framework for Features at Any Resolution" name="description">
  <meta content="FeatUp" property="og:title">
  <meta content="A Model-Agnostic Framework for Features at Any Resolution" property="og:description">
  <meta content="https://marhamilresearch4.blob.core.windows.net/anonymous-sites/images/featup_website-p-1080.png" property="og:image">
  <meta content="FeatUp" property="twitter:title">
  <meta content="A Model-Agnostic Framework for Features at Any Resolution" property="twitter:description">
  <meta content="https://marhamilresearch4.blob.core.windows.net/anonymous-sites/images/featup_website-p-1080.png" property="twitter:image">
  <meta property="og:type" content="website">
  <meta content="summary_large_image" name="twitter:card">
  <meta content="width=device-width, initial-scale=1" name="viewport">
  <meta content="Webflow" name="generator">
  <link href="css/normalize.css" rel="stylesheet" type="text/css">
  <link href="css/webflow.css" rel="stylesheet" type="text/css">
  <link href="css/mhamilton723.webflow.css" rel="stylesheet" type="text/css">
  <link href="https://fonts.googleapis.com" rel="preconnect">
  <link href="https://fonts.gstatic.com" rel="preconnect" crossorigin="anonymous">
  <script src="https://ajax.googleapis.com/ajax/libs/webfont/1.6.26/webfont.js" type="text/javascript"></script>
  <script type="text/javascript">WebFont.load({  google: {    families: ["Inconsolata:400,700","Roboto:100,300,regular,500,700"]  }});</script>
  <script type="text/javascript">!function(o,c){var n=c.documentElement,t=" w-mod-";n.className+=t+"js",("ontouchstart"in o||o.DocumentTouch&&c instanceof DocumentTouch)&&(n.className+=t+"touch")}(window,document);</script>
  <link href="images/favicon.ico" rel="shortcut icon" type="image/x-icon">
  <link href="images/webclip.png" rel="apple-touch-icon">
  <script async="" src="https://www.googletagmanager.com/gtag/js?id=G-V1K6GK8BBM"></script>
  <script type="text/javascript">window.dataLayer = window.dataLayer || [];function gtag(){dataLayer.push(arguments);}gtag('js', new Date());gtag('set', 'developer_id.dZGVlNj', true);gtag('config', 'G-V1K6GK8BBM');</script>
</head>
<body class="body">
  <div data-collapse="small" data-animation="over-right" data-duration="400" data-easing="ease" data-easing2="ease" role="banner" class="nav w-nav">
    <div class="nav-container w-container">
      <nav role="navigation" class="nav-menu w-nav-menu">
        <div class="div-block">
          <a href="#examples" class="nav-link w-nav-link">Examples</a>
          <a href="#Video" class="nav-link w-nav-link">Video</a>
          <a href="#Abstract" class="nav-link w-nav-link">Abstract</a>
          <a href="#About" class="nav-link w-nav-link">About</a>
          <a href="https://marhamilresearch4.blob.core.windows.net/feature-upsampling-public/FeatUp_ICLR_2024.pdf" class="nav-link w-nav-link">Paper</a>
          <a href="#Related-Projects" class="nav-link w-nav-link">Related Projects</a>
          <a href="#Contact" class="nav-link w-nav-link">Contact</a>
        </div>
      </nav>
      <div class="menu-button w-nav-button">
        <div class="icon w-icon-nav-menu"></div>
      </div>
    </div>
  </div>
  <div id="Hero" class="paper-hero">
    <div class="hero-wrap"></div>
    <h1 class="paper-title">FeatUp: A Model-Agnostic Framework<br>for Features at Any Resolution</h1>
    <h1 class="h2">ICLR 2024</h1>
    <div class="paper-button-holder">
      <a target="_blank" href="https://openreview.net/pdf?id=GkJiNn2QDF" class="button w-button">Paper</a>
      <a target="_blank" href="https://aka.ms/featup-code" class="button w-button">Code</a>
    </div>
    <div class="spacer"></div>
    <h1 class="paper-names">
      <a href="https://stephanie-fu.github.io/" target="_blank" class="link">Stephanie Fu*</a>, <a href="https://mhamilton.net" target="_blank" class="link">Mark Hamilton*</a>, <a href="https://people.csail.mit.edu/lebrandt/" target="_blank" class="link">Laura Brandt</a>, <a href="https://feldmann.nyc/" target="_blank" class="link">Axel Feldman</a>, <a href="https://ztzhang.info/" target="_blank" class="link">Zhoutong Zhang</a>, <a href="https://billf.mit.edu/about/bio" target="_blank" class="link">William T. Freeman</a>
    </h1>
    <div class="w-layout-grid affiliationholder"><img src="images/mit_logo_white.svg" loading="lazy" alt="" id="w-node-_43a3413d-4adf-1800-f7a1-439b5010f325-f4f8ffe1" class="affiliation-image"><img src="images/msft_logo_white.svg" loading="lazy" alt="" class="affiliation-image"><img src="images/University_of_California_Berkeley_logo.svg" loading="lazy" alt="" class="affiliation-image"><img src="images/Google_2015_logo_colorless_mourning_period.svg" loading="lazy" alt="" class="affiliation-image">
      <h1 class="equalcontribution">*Equal Contribution</h1>
    </div>
    <div class="paper-page-section-width"><img src="images/website_hero_small.jpg" loading="lazy" width="559" sizes="(max-width: 479px) 89vw, (max-width: 767px) 93vw, (max-width: 991px) 88vw, 750.0000610351562px" alt="" srcset="images/website_hero_small-p-500.jpg 500w, images/website_hero_small-p-800.jpg 800w, images/website_hero_small-p-1080.jpg 1080w, images/website_hero_small-p-1600.jpg 1600w, images/website_hero_small-p-2000.jpg 2000w, images/website_hero_small-p-2600.jpg 2600w, images/website_hero_small-p-3200.jpg 3200w, images/website_hero_small.jpg 4413w" class="paper-image">
      <div class="tldr-text"><strong>TL;DR</strong>: FeatUp improves the spatial resolution of <em>any</em> model&#x27;s features by 16-32x without changing their semantics.</div>
      <h1 id="examples" class="paper-section-header">Examples</h1>
    </div>
    <div class="paper-page-section-width">
      <div class="w-layout-grid featup-video-grid">
        <div id="w-node-_9082397f-6628-a5b0-a27a-b89c00682df9-f4f8ffe1" class="comparevideoholder">
          <div class="w-layout-hflex smallvideocaptionheader">
            <div class="caption3">Video</div>
            <div class="caption3">DINO</div>
            <div class="caption3">DINO+FeatUp</div>
          </div>
          <div id="w-node-_9082397f-6628-a5b0-a27a-b89c00682e01-f4f8ffe1" class="html-embed w-embed">
            <style>
    @media (max-width: 480px) {
        .responsive-video {
            max-height: 200px;
        }
    }
</style>
            <video playsinline="" autoplay="" loop="" preload="" muted="" class="responsive-video" style="max-width: 100%; object-fit: contain;">
              <source src="https://marhamilresearch4.blob.core.windows.net/feature-upsampling-public/smooth_feat_vids_2/dino16/davis_bmx-bumps/all_feats.mp4" type="video/mp4">
            </video>
          </div>
        </div>
        <div id="w-node-_9082397f-6628-a5b0-a27a-b89c00682e02-f4f8ffe1" class="comparevideoholder">
          <div class="w-layout-hflex smallvideocaptionheader">
            <div class="caption3">Video</div>
            <div class="caption3">DINO</div>
            <div class="caption3">DINO+FeatUp</div>
          </div>
          <div id="w-node-_9082397f-6628-a5b0-a27a-b89c00682e0a-f4f8ffe1" class="html-embed w-embed">
            <style>
    @media (max-width: 480px) {
        .responsive-video {
            max-height: 200px;
        }
    }
</style>
            <video playsinline="" autoplay="" loop="" preload="" muted="" class="responsive-video" style="max-width: 100%; object-fit: contain;">
              <source src="https://marhamilresearch4.blob.core.windows.net/feature-upsampling-public/smooth_feat_vids_2/dino16/sample_mri2/all_feats.mp4" type="video/mp4">
            </video>
          </div>
        </div>
        <div id="w-node-_9082397f-6628-a5b0-a27a-b89c00682e0b-f4f8ffe1" class="comparevideoholder">
          <div class="w-layout-hflex smallvideocaptionheader">
            <div class="caption3">Video</div>
            <div class="caption3">DINO</div>
            <div class="caption3">DINO+FeatUp</div>
          </div>
          <div id="w-node-_9082397f-6628-a5b0-a27a-b89c00682e13-f4f8ffe1" class="html-embed w-embed">
            <style>
    @media (max-width: 480px) {
        .responsive-video {
            max-height: 200px;
        }
    }
</style>
            <video playsinline="" autoplay="" loop="" preload="" muted="" class="responsive-video" style="max-width: 100%; object-fit: contain;">
              <source src="https://marhamilresearch4.blob.core.windows.net/feature-upsampling-public/smooth_feat_vids_2/dino16/davis_dog-agility/all_feats.mp4" type="video/mp4">
            </video>
          </div>
        </div>
        <div id="w-node-_9082397f-6628-a5b0-a27a-b89c00682e14-f4f8ffe1" class="comparevideoholder">
          <div class="w-layout-hflex smallvideocaptionheader">
            <div class="caption3">Video</div>
            <div class="caption3">DINO</div>
            <div class="caption3">DINO+FeatUp</div>
          </div>
          <div id="w-node-_9082397f-6628-a5b0-a27a-b89c00682e1c-f4f8ffe1" class="html-embed w-embed">
            <style>
    @media (max-width: 480px) {
        .responsive-video {
            max-height: 200px;
        }
    }
</style>
            <video playsinline="" autoplay="" loop="" preload="" muted="" class="responsive-video" style="max-width: 100%; object-fit: contain;">
              <source src="https://marhamilresearch4.blob.core.windows.net/feature-upsampling-public/smooth_feat_vids_2/dino16/sample_dubai/all_feats.mp4" type="video/mp4">
            </video>
          </div>
        </div>
        <div id="w-node-_9082397f-6628-a5b0-a27a-b89c00682e1d-f4f8ffe1" class="comparevideoholder">
          <div class="w-layout-hflex smallvideocaptionheader">
            <div class="caption3">Video</div>
            <div class="caption3">DINO</div>
            <div class="caption3">DINO+FeatUp</div>
          </div>
          <div id="w-node-_9082397f-6628-a5b0-a27a-b89c00682e25-f4f8ffe1" class="html-embed w-embed">
            <style>
    @media (max-width: 480px) {
        .responsive-video {
            max-height: 200px;
        }
    }
</style>
            <video playsinline="" autoplay="" loop="" preload="" muted="" class="responsive-video" style="max-width: 100%; object-fit: contain;">
              <source src="https://marhamilresearch4.blob.core.windows.net/feature-upsampling-public/smooth_feat_vids_2/dino16/davis_kite-surf/all_feats.mp4" type="video/mp4">
            </video>
          </div>
        </div>
        <div id="w-node-_9082397f-6628-a5b0-a27a-b89c00682e26-f4f8ffe1" class="comparevideoholder">
          <div class="w-layout-hflex smallvideocaptionheader">
            <div class="caption3">Video</div>
            <div class="caption3">DINO</div>
            <div class="caption3">DINO+FeatUp</div>
          </div>
          <div id="w-node-_9082397f-6628-a5b0-a27a-b89c00682e2e-f4f8ffe1" class="html-embed w-embed">
            <style>
    @media (max-width: 480px) {
        .responsive-video {
            max-height: 200px;
        }
    }
</style>
            <video playsinline="" autoplay="" loop="" preload="" muted="" class="responsive-video" style="max-width: 100%; object-fit: contain;">
              <source src="https://marhamilresearch4.blob.core.windows.net/feature-upsampling-public/smooth_feat_vids_2/dino16/davis_parkour/all_feats.mp4" type="video/mp4">
            </video>
          </div>
        </div>
      </div>
      <h1 class="example-subheader">Any Backbone:</h1>
      <div id="w-node-_9082397f-6628-a5b0-a27a-b89c00682e32-f4f8ffe1" class="largevideogrid">
        <div id="w-node-_9082397f-6628-a5b0-a27a-b89c00682e33-f4f8ffe1" class="largecomparevideoholder">
          <div class="w-layout-hflex smallvideocaptionheader">
            <div class="framecaption5">Video</div>
            <div class="featcaption5">ViT</div>
            <div class="featcaption5">DINO</div>
            <div class="featcaption5">DINOv2</div>
            <div class="featcaption5">CLIP</div>
            <div class="featcaption5">RN50</div>
          </div>
          <div id="w-node-_9082397f-6628-a5b0-a27a-b89c00682e41-f4f8ffe1" class="html-embed w-embed">
            <style>
    @media (max-width: 480px) {
        .responsive-video {
            max-height: 200px;
        }
    }
</style>
            <video playsinline="" autoplay="" loop="" preload="" muted="" class="responsive-video" style="max-width: 100%; object-fit: contain;">
              <source src="https://marhamilresearch4.blob.core.windows.net/feature-upsampling-public/smooth_feat_vids_2/all_models/davis_horsejump-high/pca_viz.mp4" type="video/mp4">
            </video>
          </div>
          <div class="w-layout-hflex smallvideocaptionfooter5">
            <div class="framespacer5"></div>
            <div class="featcaption5">ViT + FeatUp</div>
            <div class="featcaption5">DINO + FeatUp</div>
            <div class="featcaption5">DINOv2 + FeatUp</div>
            <div class="featcaption5">CLIP + FeatUp</div>
            <div class="featcaption5">RN50 + FeatUp</div>
          </div>
        </div>
      </div>
      <h1 class="example-subheader">Improve Downstream Tasks without Retraining :</h1>
      <div class="largevideogrid">
        <div id="w-node-_9082397f-6628-a5b0-a27a-b89c00682e51-f4f8ffe1" class="largecomparevideoholder">
          <div class="w-layout-hflex smallvideocaptionheader">
            <div class="framecaption4">Video</div>
            <div class="featcaption4">Semseg Probe</div>
            <div class="featcaption4">Depth Probe</div>
            <div class="featcaption4">CAM (Horse)</div>
            <div class="featcaption4">CAM (Bars)</div>
          </div>
          <div id="w-node-_9082397f-6628-a5b0-a27a-b89c00682e5d-f4f8ffe1" class="html-embed w-embed">
            <style>
    @media (max-width: 480px) {
        .responsive-video {
            max-height: 200px;
        }
    }
</style>
            <video playsinline="" autoplay="" loop="" preload="" muted="" class="responsive-video" style="max-width: 100%; object-fit: contain;">
              <source src="https://marhamilresearch4.blob.core.windows.net/feature-upsampling-public/smooth_feat_vids_2/vit/davis_horsejump-high/probe_viz.mp4" type="video/mp4">
            </video>
          </div>
          <div class="w-layout-hflex smallvideocaptionfooter4">
            <div class="framespacer4"></div>
            <div class="featcaption4">Semseg + FeatUp</div>
            <div class="featcaption4">Depth + FeatUp</div>
            <div class="featcaption4">CAM + FeatUp</div>
            <div class="featcaption4">CAM + FeatUp</div>
          </div>
        </div>
      </div>
      <h1 class="example-subheader">Upsamples Every Feature Dimension:</h1>
      <div class="largevideogrid">
        <div id="w-node-_9082397f-6628-a5b0-a27a-b89c00682e6b-f4f8ffe1" class="largecomparevideoholder">
          <div class="w-layout-hflex smallvideocaptionheader">
            <div class="framecaption3">Video</div>
            <div class="featcaption3">DINO PCA 1,2,3</div>
            <div class="featcaption3">DINO PCA 4,5,6</div>
            <div class="featcaption3">DINO PCA 7,8,9</div>
          </div>
          <div id="w-node-_9082397f-6628-a5b0-a27a-b89c00682e75-f4f8ffe1" class="html-embed w-embed">
            <style>
    @media (max-width: 480px) {
        .responsive-video {
            max-height: 200px;
        }
    }
</style>
            <video playsinline="" autoplay="" loop="" preload="" muted="" class="responsive-video" style="max-width: 100%; object-fit: contain;">
              <source src="https://marhamilresearch4.blob.core.windows.net/feature-upsampling-public/smooth_feat_vids_2/dino16/davis_horsejump-high/pca_viz.mp4" type="video/mp4">
            </video>
          </div>
          <div class="w-layout-hflex smallvideocaptionfooter3">
            <div class="framespacer3"></div>
            <div class="featcaption3">FeatUp PCA 1,2,3</div>
            <div class="featcaption3">FeatUp PCA  4,5,6</div>
            <div class="featcaption3">FeatUp PCA 7,8,9</div>
          </div>
        </div>
      </div>
    </div>
  </div>
  <div id="Video" class="section">
    <div class="section-margin">
      <div class="paper-page-section-width">
        <div class="youtube w-embed"><video controls="" width="100%">
            <source src="https://dl.dropboxusercontent.com/scl/fi/3kjk4f9qfuqd35ryi2du4/featup_iclr.mp4?rlkey=w4jliueg3x0qw0kansaamh68c&dl=0" type="video/mp4">
          </video></div>
      </div>
    </div>
  </div>
  <div id="Abstract" class="section">
    <div class="section-margin">
      <div class="paper-page-section-width">
        <div class="small-spacer"></div>
        <h1 id="Abstract" class="paper-page-text-block">Deep features are a cornerstone of computer vision research, capturing image semantics and enabling the community to solve downstream tasks even in the zero- or few-shot regime. However, these features often lack the spatial resolution to directly perform dense prediction tasks like segmentation and depth prediction because models aggressively pool information over large areas. In this work, we introduce FeatUp, a task- and model-agnostic framework to restore lost spatial information in deep features. We introduce two variants of FeatUp: one that guides features with high-resolution signal in a single forward pass, and one that fits an implicit model to a single image to reconstruct features at any resolution. Both approaches use a multi-view consistency loss with deep analogies to NeRFs. Our features retain their original semantics and can be swapped into existing applications to yield resolution and performance gains even without re-training. We show that FeatUp significantly outperforms other feature upsampling and image super-resolution approaches in class activation map generation, transfer learning for segmentation and depth prediction, and end-to-end training for semantic segmentation.</h1>
      </div>
    </div>
  </div>
  <div id="About" class="section">
    <div class="section-margin">
      <div class="paper-page-section-width">
        <h1 class="paper-section-header">FeatUp: Upsampling Model Representations with Self-Supervision</h1>
        <div class="small-spacer"></div>
        <h1 class="paper-page-text-block">FeatUp upsamples any deep network&#x27;s features to <em>arbitrary</em> resolution while retaining the original semantics. We learn a high-res feature map by enforcing consistency across many low-res &quot;views&quot;, which are formed by perturbing and featurizing the input image. <br><br>Inspired by <a href="https://arxiv.org/abs/2003.08934" target="_blank" class="link">NeRF</a>&#x27;s implicit scene representation learned by enforcing image consistency across multiple views, we learn a view-consistent implicit network that outputs features at any queried resolution. This upsampler can also be parameterized as a feedforward module, usable in any existing pipeline and trainable end-to-end.</h1><img src="images/website_featup_website_small.jpg" loading="lazy" sizes="(max-width: 479px) 93vw, (max-width: 767px) 95vw, (max-width: 991px) 90vw, 750.0000610351562px" srcset="images/website_featup_website_small-p-500.jpg 500w, images/website_featup_website_small-p-800.jpg 800w, images/website_featup_website_small-p-1080.jpg 1080w, images/website_featup_website_small-p-1600.jpg 1600w, images/website_featup_website_small-p-2000.jpg 2000w, images/website_featup_website_small-p-2600.jpg 2600w, images/website_featup_website_small.jpg 3634w" alt="" class="paper-image">
      </div>
      <div class="paper-page-section-width">
        <h1 class="paper-section-header">Results</h1>
        <div class="small-spacer"></div><img src="images/website_featup_pca_website_small.jpg" loading="lazy" width="1000" sizes="(max-width: 479px) 93vw, (max-width: 767px) 95vw, (max-width: 991px) 90vw, 750.0000610351562px" alt="" srcset="images/website_featup_pca_website_small-p-500.jpg 500w, images/website_featup_pca_website_small-p-800.jpg 800w, images/website_featup_pca_website_small-p-1080.jpg 1080w, images/website_featup_pca_website_small-p-1600.jpg 1600w, images/website_featup_pca_website_small-p-2000.jpg 2000w, images/website_featup_pca_website_small-p-2600.jpg 2600w, images/website_featup_pca_website_small-p-3200.jpg 3200w, images/website_featup_pca_website_small.jpg 4308w" class="paper-image">
        <h1 class="paper-page-text-block">Above: both variants of FeatUp (implicit and feedforward JBU module) resolve high-res details that other methods cannot. Additionally our features lie in the same space as the input features, making them usable in downstream architectures without re-training.<br>‍</h1><img src="images/website_backbone_comp_small.jpg" loading="lazy" sizes="(max-width: 479px) 93vw, (max-width: 767px) 95vw, (max-width: 991px) 90vw, 750.0000610351562px" srcset="images/website_backbone_comp_small-p-500.jpg 500w, images/website_backbone_comp_small-p-800.jpg 800w, images/website_backbone_comp_small-p-1080.jpg 1080w, images/website_backbone_comp_small-p-1600.jpg 1600w, images/website_backbone_comp_small-p-2000.jpg 2000w, images/website_backbone_comp_small-p-2600.jpg 2600w, images/website_backbone_comp_small-p-3200.jpg 3200w, images/website_backbone_comp_small.jpg 3817w" alt="" class="paper-image">
        <h1 class="paper-page-text-block"><br>Above: Upsampled features from a variety of vision backbones. FeatUp introduces spatial resolution while preserving semantics.</h1>
      </div>
      <div class="paper-page-section-width">
        <h1 class="paper-section-header">Downstream Evaluations</h1>
        <div class="small-spacer"></div>
        <h1 class="paper-page-text-block">We evaluate FeatUp on a variety of downstream tasks from the broader literature, including linear probe transfer learning, where features are directly used for depth estimation and semantic segmentation. Additionally, we evaluate CAM quality and weakly-supervised object localization performance. Across the board, our methods qualitatively and quantitatively improve performance on these downstream tasks - see our supplementary material for more examples.</h1><img src="images/featup_downstream_website.png" loading="lazy" width="2456.5" sizes="(max-width: 479px) 93vw, (max-width: 767px) 95vw, (max-width: 991px) 90vw, 750.0000610351562px" alt="" srcset="images/featup_downstream_website-p-500.png 500w, images/featup_downstream_website-p-800.png 800w, images/featup_downstream_website-p-1080.png 1080w, images/featup_downstream_website-p-1600.png 1600w, images/featup_downstream_website-p-2000.png 2000w, images/featup_downstream_website-p-2600.png 2600w, images/featup_downstream_website-p-3200.png 3200w, images/featup_downstream_website.png 4913w" class="paper-image">
      </div>
    </div>
  </div>
  <div id="Paper" class="section">
    <div class="section-margin">
      <div class="paper-page-section-width">
        <h1 class="paper-section-header">Paper</h1>
        <div class="small-spacer"></div>
        <a href="https://openreview.net/pdf?id=GkJiNn2QDF" target="_blank" class="w-inline-block"><img src="images/paper_panel.jpg" loading="lazy" sizes="100vw" srcset="images/paper_panel-p-500.jpg 500w, images/paper_panel-p-800.jpg 800w, images/paper_panel-p-1080.jpg 1080w, images/paper_panel-p-1600.jpg 1600w, images/paper_panel.jpg 2000w" alt="" class="paper-thumbnail"></a>
      </div>
      <div class="paper-page-section-width">
        <h1 class="paper-section-header">Bibtex</h1>
        <div class="small-spacer"></div>
        <div class="code-block">@inproceedings{<br>    fu2024featup,<br>    title={FeatUp: A Model-Agnostic Framework for Features at Any Resolution},<br>    author={Stephanie Fu and Mark Hamilton and Laura E. Brandt and Axel Feldmann and Zhoutong Zhang and William T. Freeman},<br>    booktitle={The Twelfth International Conference on Learning Representations},<br>    year={2024},<br>    url={https://openreview.net/forum?id=GkJiNn2QDF}<br>}</div>
      </div>
    </div>
  </div>
  <div id="Related-Projects" class="section">
    <div class="section-margin">
      <div class="paper-page-section-width">
        <h1 class="paper-section-header">Related Projects</h1>
        <div class="small-spacer"></div>
        <div class="paper-entry">
          <div class="pub-image-div"><img src="images/ezgif.com-gif-maker-3.gif" width="436" alt="" loading="lazy" class="pub-img"></div>
          <div class="pub-item">
            <h2 class="paper-titile">Unsupervised Semantic Segmentation by Distilling Feature Correspondences<br></h2>
            <div class="paper-buttons">
              <a target="_blank" href="stego.html" class="button w-button">Website</a>
              <a target="_blank" href="https://arxiv.org/abs/2203.08414" class="button w-button">Paper</a>
              <a target="_blank" href="https://iclr.cc/virtual/2022/poster/6068" class="button w-button">Talk</a>
              <a target="_blank" href="https://github.com/mhamilton723/STEGO" class="button w-button">Github</a>
              <a target="_blank" href="https://raw.githubusercontent.com/mhamilton723/mhamilton723.github.io/master/files/bib/hamilton2022unsupervised.bib" class="button w-button">BibTex</a>
            </div>
            <h1 class="author-list">We show that inner products between deep features hold a key to solving unsupervised semantic segmentation. In particular we distill these features into high quality unsupervised semantic segmentaions.</h1>
          </div>
        </div>
        <div class="paper-entry last-entry">
          <div class="pub-image-div">
            <div class="pub-img w-embed"><video width="100%" playsinline="" autoplay="" loop="" preload="" muted="">
                <source src="https://mhamilton.net/videos/projection.mp4" type="video/mp4">
              </video></div>
          </div>
          <div class="pub-item">
            <h2 class="paper-titile">Axiomatic Explanations for Visual Search, Retrieval, and Similarity Learning<br></h2>
            <div class="paper-buttons">
              <a target="_blank" href="#" class="button w-button">Website</a>
              <a target="_blank" href="https://arxiv.org/abs/2103.00370" class="button w-button">Paper</a>
              <a target="_blank" href="https://iclr.cc/virtual/2022/poster/6983" class="button w-button">Talk</a>
              <a target="_blank" href="https://raw.githubusercontent.com/mhamilton723/mhamilton723.github.io/master/files/bib/hamilton2021model.bib" class="button w-button">BibTex</a>
            </div>
            <h1 class="author-list">We show that inner products between deep vision features can be interpreted as a generalization of Shapley Values for contrastive image similarity networks. We explore this theory and show that it provides a unique axiomatic characterization of contrastive model explanation methods.</h1>
          </div>
        </div>
      </div>
    </div>
  </div>
  <div id="Contact" class="section">
    <div class="section-margin">
      <div class="paper-page-section-width">
        <h1 class="paper-section-header">Contact</h1>
        <div class="small-spacer"></div>
        <h1 class="paper-page-text-block">For feedback, questions, or press inquiries please contact <a href="mailto:markth@mit.edu?subject=FeatUp" class="link">Mark Hamilton</a> and <a href="mailto:fus@mit.edu?subject=FeatUp" class="link">Stephanie Fu</a>
        </h1>
      </div>
      <div class="footer"></div>
    </div>
  </div>
  <script src="https://d3e54v103j8qbb.cloudfront.net/js/jquery-3.5.1.min.dc5e7f18c8.js?site=5fd97fcae7aaf2c02444f619" type="text/javascript" integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin="anonymous"></script>
  <script src="js/webflow.js" type="text/javascript"></script>
  <!-- <script async src="lib/particles.min.js"></script>  -->
</body>
</html>