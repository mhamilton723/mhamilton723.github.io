<!DOCTYPE html><!--  This site was created in Webflow. https://www.webflow.com  -->
<!--  Last Published: Wed Jan 03 2024 01:24:43 GMT+0000 (Coordinated Universal Time)  -->
<html data-wf-page="624d23ebb2ee8c7b68d1e30c" data-wf-site="5fd97fcae7aaf2c02444f619">
<head>
  <meta charset="utf-8">
  <title>Axiomatic Explanations for Visual Search, Retrieval, and Similarity Learning</title>
  <meta content="Understanding the predictions of search engines, contrastive learning, and recommender systems with higher-order generalizations of the Shapley Value." name="description">
  <meta content="Axiomatic Explanations for Visual Search, Retrieval, and Similarity Learning" property="og:title">
  <meta content="Understanding the predictions of search engines, contrastive learning, and recommender systems with higher-order generalizations of the Shapley Value." property="og:description">
  <meta content="https://mhamilton.net/images/axiomatic_video_header.jpg" property="og:image">
  <meta content="Axiomatic Explanations for Visual Search, Retrieval, and Similarity Learning" property="twitter:title">
  <meta content="Understanding the predictions of search engines, contrastive learning, and recommender systems with higher-order generalizations of the Shapley Value." property="twitter:description">
  <meta content="https://mhamilton.net/images/axiomatic_video_header.jpg" property="twitter:image">
  <meta property="og:type" content="website">
  <meta content="summary_large_image" name="twitter:card">
  <meta content="width=device-width, initial-scale=1" name="viewport">
  <meta content="Webflow" name="generator">
  <link href="css/normalize.css" rel="stylesheet" type="text/css">
  <link href="css/webflow.css" rel="stylesheet" type="text/css">
  <link href="css/mhamilton723.webflow.css" rel="stylesheet" type="text/css">
  <link href="https://fonts.googleapis.com" rel="preconnect">
  <link href="https://fonts.gstatic.com" rel="preconnect" crossorigin="anonymous">
  <script src="https://ajax.googleapis.com/ajax/libs/webfont/1.6.26/webfont.js" type="text/javascript"></script>
  <script type="text/javascript">WebFont.load({  google: {    families: ["Inconsolata:400,700","Roboto:100,300,regular,500,700"]  }});</script>
  <script type="text/javascript">!function(o,c){var n=c.documentElement,t=" w-mod-";n.className+=t+"js",("ontouchstart"in o||o.DocumentTouch&&c instanceof DocumentTouch)&&(n.className+=t+"touch")}(window,document);</script>
  <link href="images/favicon.ico" rel="shortcut icon" type="image/x-icon">
  <link href="images/webclip.png" rel="apple-touch-icon">
  <script async="" src="https://www.googletagmanager.com/gtag/js?id=G-V1K6GK8BBM"></script>
  <script type="text/javascript">window.dataLayer = window.dataLayer || [];function gtag(){dataLayer.push(arguments);}gtag('js', new Date());gtag('set', 'developer_id.dZGVlNj', true);gtag('config', 'G-V1K6GK8BBM');</script>
</head>
<body class="body">
  <div data-collapse="small" data-animation="over-right" data-duration="400" data-easing="ease" data-easing2="ease" role="banner" class="nav w-nav">
    <div class="nav-container w-container">
      <nav role="navigation" class="nav-menu w-nav-menu">
        <div class="div-block">
          <a href="#Video" class="nav-link w-nav-link">Video</a>
          <a href="#Abstract" class="nav-link w-nav-link">Abstract</a>
          <a href="#About" class="nav-link w-nav-link">About</a>
          <a href="#Paper" class="nav-link w-nav-link">Paper</a>
          <a href="#Related-Projects" class="nav-link w-nav-link">Related Projects</a>
          <a href="#Contact" class="nav-link w-nav-link">Contact</a>
        </div>
      </nav>
      <div class="menu-button w-nav-button">
        <div class="icon w-icon-nav-menu"></div>
      </div>
    </div>
  </div>
  <div id="Hero" class="paper-hero">
    <div class="hero-wrap"></div>
    <h1 class="paper-title">Axiomatic Explanations for Visual Search,<br>Retrieval, and Similarity Learning</h1>
    <h1 class="h2">ICLR 2022</h1>
    <div class="paper-button-holder">
      <a target="_blank" href="https://arxiv.org/abs/2103.00370" class="button w-button">Paper</a>
      <a target="_blank" href="https://aka.ms/axiomatic-code" class="button w-button">Code</a>
      <a target="_blank" href="https://iclr.cc/virtual/2022/poster/6983" class="button w-button">Talk</a>
    </div>
    <div class="spacer"></div>
    <h1 class="paper-names">
      <a href="https://mhamilton.net" target="_blank" class="link">Mark Hamilton</a>, <a href="https://scottlundberg.com/" target="_blank" class="link">Scott Lundberg</a>, <a href="https://www.leizhang.org/" target="_blank" class="link">Lei Zhang</a>, <a href="https://www.mit.edu/~fus/" target="_blank" class="link">Stephanie Fu</a>, <a href="https://billf.mit.edu/about/bio" target="_blank" class="link">William T. Freeman</a>
    </h1>
    <div class="w-layout-grid affiliation-grid-3panel"><img src="images/mit_logo_white.svg" loading="lazy" alt="" id="w-node-_185912c3-010d-fb94-8b11-ceb325f173f4-68d1e30c" class="affiliation-image"><img src="images/msft_logo_white.svg" loading="lazy" alt="" class="affiliation-image"><img src="images/Google_2015_logo_colorless_mourning_period.svg" loading="lazy" alt="" class="affiliation-image"></div>
  </div>
  <div id="Video" class="section">
    <div class="section-margin">
      <div class="paper-page-section-width">
        <div style="padding-top:56.17021276595745%" class="w-embed-youtubevideo youtube"><iframe src="https://www.youtube.com/embed/dKO_qhGEN_I?rel=0&amp;controls=1&amp;autoplay=0&amp;mute=0&amp;start=0" frameborder="0" style="position:absolute;left:0;top:0;width:100%;height:100%;pointer-events:auto" allow="autoplay; encrypted-media" allowfullscreen="" title="Axiomatic Explanations for Visual Search, Retrieval, and Similarity Learning"></iframe></div>
      </div>
    </div>
  </div>
  <div id="Abstract" class="section">
    <div class="section-margin">
      <div class="paper-page-section-width">
        <div class="small-spacer"></div>
        <h1 class="paper-page-text-block">Visual search, recommendation, and contrastive similarity learning power technologies that impact billions of users worldwide. Modern model architectures can be complex and difficult to interpret, and there are several competing techniques one can use to explain a search engine&#x27;s behavior. We show that the theory of fair credit assignment provides a <strong>unique</strong> axiomatic solution that generalizes several existing recommendation- and metric-explainability techniques in the literature. Using this formalism, we show when existing approaches violate &quot;fairness&quot; and derive methods that sidestep these shortcomings and naturally handle counterfactual information. More specifically, we show existing approaches implicitly approximate second-order Shapley-Taylor indices and extend CAM, GradCAM, LIME, SHAP, SBSM, and other methods to search engines. These extensions can extract pairwise correspondences between images from trained <strong>opaque-box</strong> models. We also introduce a fast kernel-based method for estimating Shapley-Taylor indices that require orders of magnitude fewer function evaluations to converge. Finally, we show that these game-theoretic measures yield more consistent explanations for image similarity architectures.</h1>
      </div>
    </div>
  </div>
  <div id="About" class="section">
    <div class="section-margin">
      <div class="paper-page-section-width">
        <h1 class="paper-section-header">Explaining search, contrastive learning, and recommendation systems</h1>
        <div class="small-spacer"></div>
        <h1 class="paper-page-text-block">Search, recommendation, retrieval, and contrastive similarity learning help us organize information at scales that no human could match. The recent surge in million and billion parameter contrastive architectures for vision and language underscore the growing need to understand these classes of systems. This work identifies two distinct classes of explainability methods for these systems. &quot;First order&quot; approaches <strong>highlight the most important pixels</strong> that contribute to the similarity of objects and &quot;Second order&#x27;&#x27; explanations provide a <strong>full correspondence</strong> between the parts of query and retrieved image. Our approaches allow one to understand the predictions and similarity judgements of contrastive architectures <strong>without Â access</strong> to the model weights (opaque-box).</h1><img src="images/search_exp_arch.jpg" loading="lazy" sizes="(max-width: 767px) 95vw, (max-width: 991px) 90vw, 50vw" srcset="images/search_exp_arch-p-500.jpeg 500w, images/search_exp_arch-p-1080.jpeg 1080w, images/search_exp_arch-p-1600.jpeg 1600w, images/search_exp_arch.jpg 2000w" alt="" class="image-2">
      </div>
      <div class="paper-page-section-width">
        <h1 class="paper-section-header">Generalizing Shapley Values with the Shapley-Taylor Indices</h1>
        <div class="small-spacer"></div>
        <h1 class="paper-page-text-block">
          <a href="https://proceedings.neurips.cc/paper/2017/hash/8a20a8621978632d76c43dfd28b67767-Abstract.html" target="_blank" class="link">Shapley Values</a> connect the fields of economics and machine learning and provide a principled framework for explaining classifiers and regressors. In economics, these values are the <strong>unique, fair way</strong> to pay employees based on their contributions. In machine learning, these values provide the <strong>only fair way</strong> to attribute a model&#x27;s predictions to input features. We show that this connection between economics and ML extends to a generalization of the Shapley Values called the <a href="https://arxiv.org/abs/1902.05622" target="_blank" class="link">Shapley-Taylor Index</a>. In economics, this allows us to calculate fair payments for groups of employees. In ML, this measures the interaction between features enabling the <strong>extraction of dense image correspondences</strong> from search, contrastive, and recommendation systems <strong>without knowing the inner workings of the model</strong>.
        </h1>
        <div class="image-2-copy w-embed"><video width="100%" playsinline="" autoplay="" loop="" preload="" muted="">
            <source src="https://mhamilton.net/videos/harsanyi.mp4" type="video/mp4">
          </video></div>
      </div>
      <div class="paper-page-section-width">
        <h1 class="paper-section-header">Fast Approximations of Shapley-Taylor Indices</h1>
        <div class="small-spacer"></div>
        <h1 class="paper-page-text-block">Though Shapley Values and Shapley-Taylor indices are the unique measures we can uses to understand models, they are difficult to compute for nonlinear models. We show that existing methods such as <strong>CAM, GradCAM, and LIME are implicitly approximating these values</strong>, and use this to design new generalizations of these methods for contrastive, search, and recommendation architectures. When generalizing LIME to handle higher-order interactions, we discovered a new estimator for Shapley-Taylor indices using Kernel-weighted quadratic regression which converges with <strong>10x fewer function evaluations</strong>.</h1><img src="images/unifying_methods.svg" loading="lazy" alt="" class="image-4">
      </div>
    </div>
  </div>
  <div id="Paper" class="section">
    <div class="section-margin">
      <div class="paper-page-section-width">
        <h1 class="paper-section-header">Paper</h1>
        <div class="small-spacer"></div>
        <a href="https://arxiv.org/abs/2203.08414" target="_blank" class="w-inline-block"><img src="images/axiomatic_paper_spread.jpg" loading="lazy" sizes="100vw" srcset="images/axiomatic_paper_spread-p-500.jpeg 500w, images/axiomatic_paper_spread-p-800.jpeg 800w, images/axiomatic_paper_spread-p-1080.jpeg 1080w, images/axiomatic_paper_spread-p-1600.jpeg 1600w, images/axiomatic_paper_spread.jpg 2689w" alt="" class="paper-thumbnail"></a>
      </div>
      <div class="paper-page-section-width">
        <h1 class="paper-section-header">Bibtex</h1>
        <div class="small-spacer"></div>
        <div class="code-block">@article{hamilton2021axiomatic,<br> Â Â title={Axiomatic Explanations for Visual Search, Retrieval, and Similarity Learning},<br>Â Â  author={Hamilton, Mark and Lundberg, Scott and Zhang, Lei and Fu, Stephanie and Freeman, William T},<br> Â  journal={arXiv preprint arXiv:2103.00370},<br>Â  Â year={2021}<br>}</div>
      </div>
    </div>
  </div>
  <div id="Related-Projects" class="section">
    <div class="section-margin">
      <div class="paper-page-section-width">
        <h1 class="paper-section-header">Related Projects</h1>
        <div class="small-spacer"></div>
        <div class="paper-entry-2">
          <div class="pub-image-div"><img src="images/ezgif.com-gif-maker-3.gif" width="436" alt="" loading="lazy" class="pub-img"></div>
          <div class="pub-item">
            <h2 class="paper-titile">Unsupervised Semantic Segmentation by Distilling Feature Correspondences<br></h2>
            <div class="paper-buttons">
              <a target="_blank" href="stego.html" class="button w-button">Website</a>
              <a target="_blank" href="https://arxiv.org/abs/2203.08414" class="button w-button">Paper</a>
              <a target="_blank" href="https://iclr.cc/virtual/2022/poster/6068" class="button w-button">Talk</a>
              <a target="_blank" href="https://github.com/mhamilton723/STEGO" class="button w-button">Github</a>
              <a target="_blank" href="https://raw.githubusercontent.com/mhamilton723/mhamilton723.github.io/master/files/bib/hamilton2022unsupervised.bib" class="button w-button">BibTex</a>
            </div>
            <h1 class="author-list">Second order contrastive model explanations extract dense pairwise correspondences between images that connect semantically similar objects together across a dataset. We distill these correspondences to jointly discover objects and semantically segment images without labels in any step of the pipeline.</h1>
          </div>
        </div>
        <div class="paper-entry-2">
          <div class="pub-image-div">
            <div class="pub-img w-embed"><video width="100%" playsinline="" autoplay="" loop="" preload="" muted="">
                <source src="https://mhamilton.net/videos/CIR.mp4" type="video/mp4">
              </video></div>
          </div>
          <div class="pub-item">
            <h2 class="paper-titile">MosAIc: Finding Artistic Connections across Culture with Conditional Image Retrieval</h2>
            <div class="paper-buttons">
              <a target="_blank" href="https://arxiv.org/abs/2007.07177" class="button w-button">Paper</a>
              <a target="_blank" href="http://www.aka.ms/mosaic" class="button w-button">Website</a>
              <a target="_blank" href="https://note.microsoft.com/MSR-Webinar-Visual-Analogies-Registration-On-Demand.html" class="button w-button">Webinar</a>
              <a target="_blank" href="https://youtu.be/y0iYKr9ayfE" class="button w-button">Talk</a>
              <a target="_blank" href="https://github.com/microsoft/art" class="button w-button">Code</a>
              <a target="_blank" href="https://raw.githubusercontent.com/mhamilton723/mhamilton723.github.io/master/files/bib/hamilton2020conditional.bib" class="button w-button">BibTex</a>
            </div>
            <h1 class="author-list">We introduce a new K-Nearest Neighbor data-structure to enable fast computation of an images conditional nearest neighbors in deep feature space. We show the approach can find &quot;hidden connections&quot; in the visual arts, as well as &quot;blind-spots&quot; in trained Generative AdversarialÂ Networks.</h1>
          </div>
        </div>
      </div>
    </div>
  </div>
  <div id="Related-Projects" class="section">
    <div id="Contact" class="section-margin">
      <div class="paper-page-section-width">
        <h1 class="paper-section-header">Contact</h1>
        <div class="small-spacer"></div>
        <h1 class="paper-page-text-block">For feedback, questions, or press inquiries please contact <a href="mailto:markth@mit.edu?subject=Axiomatic%20Explanations%20for%20Search" class="link">Mark Hamilton</a>
        </h1>
      </div>
      <div class="footer"></div>
    </div>
  </div>
  <script src="https://d3e54v103j8qbb.cloudfront.net/js/jquery-3.5.1.min.dc5e7f18c8.js?site=5fd97fcae7aaf2c02444f619" type="text/javascript" integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin="anonymous"></script>
  <script src="js/webflow.js" type="text/javascript"></script>
  <!-- <script async src="lib/particles.min.js"></script>  -->
</body>
</html>