<!DOCTYPE html><!--  This site was created in Webflow. https://webflow.com  --><!--  Last Published: Wed Apr 23 2025 14:51:44 GMT+0000 (Coordinated Universal Time)  -->
<html data-wf-page="67edbb0ef869fec7d7133576" data-wf-site="5fd97fcae7aaf2c02444f619">
<head>
  <meta charset="utf-8">
  <title>I-Con: A Unifying Framework for Representation Learning</title>
  <meta content="Introducing a Periodic Table of Machine Learning Algorithms" name="description">
  <meta content="I-Con: A Unifying Framework for Representation Learning" property="og:title">
  <meta content="Introducing a Periodic Table of Machine Learning Algorithms" property="og:description">
  <meta content="https://mhamilton.net/images/icon_og.svg" property="og:image">
  <meta content="I-Con: A Unifying Framework for Representation Learning" property="twitter:title">
  <meta content="Introducing a Periodic Table of Machine Learning Algorithms" property="twitter:description">
  <meta content="https://mhamilton.net/images/icon_og.svg" property="twitter:image">
  <meta property="og:type" content="website">
  <meta content="summary_large_image" name="twitter:card">
  <meta content="width=device-width, initial-scale=1" name="viewport">
  <meta content="Webflow" name="generator">
  <link href="css/normalize.css" rel="stylesheet" type="text/css">
  <link href="css/webflow.css" rel="stylesheet" type="text/css">
  <link href="css/mhamilton723.webflow.css" rel="stylesheet" type="text/css">
  <link href="https://fonts.googleapis.com" rel="preconnect">
  <link href="https://fonts.gstatic.com" rel="preconnect" crossorigin="anonymous">
  <script src="https://ajax.googleapis.com/ajax/libs/webfont/1.6.26/webfont.js" type="text/javascript"></script>
  <script type="text/javascript">WebFont.load({  google: {    families: ["Inconsolata:400,700","Roboto:100,300,regular,500,700"]  }});</script>
  <script type="text/javascript">!function(o,c){var n=c.documentElement,t=" w-mod-";n.className+=t+"js",("ontouchstart"in o||o.DocumentTouch&&c instanceof DocumentTouch)&&(n.className+=t+"touch")}(window,document);</script>
  <link href="images/favicon.ico" rel="shortcut icon" type="image/x-icon">
  <link href="images/webclip.png" rel="apple-touch-icon">
  <script async="" src="https://www.googletagmanager.com/gtag/js?id=G-V1K6GK8BBM"></script>
  <script type="text/javascript">window.dataLayer = window.dataLayer || [];function gtag(){dataLayer.push(arguments);}gtag('js', new Date());gtag('set', 'developer_id.dZGVlNj', true);gtag('config', 'G-V1K6GK8BBM');</script>
</head>
<body class="body">
  <div data-collapse="small" data-animation="over-right" data-duration="400" data-easing="ease" data-easing2="ease" role="banner" class="nav w-nav">
    <div class="nav-container w-container">
      <nav role="navigation" class="nav-menu w-nav-menu">
        <div class="div-block">
          <a href="#Video" class="nav-link w-nav-link">Video</a>
          <a href="#Abstract" class="nav-link w-nav-link">Abstract</a>
          <a href="#About" class="nav-link w-nav-link">About</a>
          <a href="#Paper" class="nav-link w-nav-link">Paper</a>
          <a href="#Related-Projects" class="nav-link w-nav-link">Related Projects</a>
          <a href="#Contact" class="nav-link w-nav-link">Contact</a>
        </div>
      </nav>
      <div class="menu-button w-nav-button">
        <div class="icon w-icon-nav-menu"></div>
      </div>
    </div>
  </div>
  <div id="Hero" class="paper-hero">
    <div class="hero-wrap"></div>
    <h1 class="paper-title">I-Con: A Unifying Framework for Representation Learning</h1>
    <h1 class="h2">ICLR 2025</h1>
    <div class="paper-button-holder">
      <a target="_blank" href="https://aka.ms/icon-paper" class="button w-button">Paper</a>
      <a target="_blank" href="https://aka.ms/icon-code" class="button w-button">Code</a>
    </div>
    <div class="spacer"></div>
    <h1 class="paper-names">
      <a href="https://shadealsha.github.io/" target="_blank" class="link">Shaden Alshammari</a>, <a href="https://research.google/people/106072/?&amp;type=google" target="_blank" class="link">John Hershey</a>, <a href="https://feldmann.nyc/" target="_blank" class="link">Axel Feldmann</a>, <a href="https://billf.mit.edu/about/bio" target="_blank" class="link">William T. Freeman,</a>
      <a href="https://mhamilton.net" target="_blank" class="link">Mark Hamilton</a> 
    </h1>
    <div class="w-layout-grid affiliationholder"><img src="images/mit_logo_white.svg" loading="lazy" alt="" id="w-node-_185912c3-010d-fb94-8b11-ceb325f173f4-d7133576" class="affiliation-image"><img src="images/msft_logo_white.svg" loading="lazy" alt="" class="affiliation-image"><img src="images/Google_2015_logo_colorless_mourning_period.svg" loading="lazy" alt="" class="affiliation-image"></div>
    <div class="w-layout-blockcontainer paper-page-section-width w-container"><img src="images/periodic_table.svg" loading="lazy" alt="" class="image-4">
      <div class="tldr-text"><strong>TL;DR</strong>: We introduce a single equation that unifies &gt;20 machine learning methods into a <strong><em>periodic table</em></strong>. We use this framework to make a state-of-the-art unsupervised image classifier.</div>
    </div>
  </div>
  <div id="Video" class="section">
    <div class="section-margin">
      <div class="paper-page-section-width">
        <div style="padding-top:56.17021276595745%" class="w-embed-youtubevideo"><iframe src="https://www.youtube.com/embed/UvjTbnFzRac?rel=0&amp;controls=1&amp;autoplay=0&amp;mute=0&amp;start=0" frameborder="0" style="position:absolute;left:0;top:0;width:100%;height:100%;pointer-events:auto" allow="autoplay; encrypted-media" allowfullscreen="" title="I-Con: A Unifying Framework for Representation Learning (ICLR 2025)"></iframe></div>
      </div>
    </div>
  </div>
  <div id="Abstract" class="section">
    <div class="section-margin">
      <div class="paper-page-section-width">
        <div class="small-spacer"></div>
        <h1 class="paper-page-text-block">As the field of representation learning grows, there has been a proliferation of different loss functions to solve different classes of problems. <strong>We introduce a single information-theoretic equation that generalizes a large collection of modern loss functions</strong> in machine learning. In particular, we introduce a framework that shows that several broad classes of machine learning methods are precisely minimizing an integrated KL divergence between two conditional distributions: the supervisory and learned representations. This viewpoint exposes a hidden information geometry underlying clustering, spectral methods, dimensionality reduction, contrastive learning, and supervised learning. This framework enables the development of new loss functions by combining successful techniques from across the literature. We not only present a wide array of proofs, <strong>connecting over 23 different approaches</strong>, but we also leverage these theoretical results to create state-of-the-art unsupervised image classifiers that <strong>achieve a +8% improvement</strong> over the prior state-of-the-art on unsupervised classification on ImageNet-1K. We also demonstrate that I-Con can be used to derive principled debiasing methods which improve contrastive representation learners.</h1>
      </div>
    </div>
  </div>
  <div id="About" class="section">
    <div class="section-margin">
      <div class="paper-page-section-width">
        <h1 class="paper-section-header">Information Contrastive Learning (I-Con)</h1>
        <div class="small-spacer"></div>
        <h1 class="paper-page-text-block">Machine learning methods can seem like a collection of isolated techniques, but what if they all shared a deeper connection? Information Contrastive Learning (I-Con) uncovers this unity. At its core, I-Con reframes over 20 different machine learning methods as the problem of approximating relationships in a training dataset.  Mathematically, the I-Con loss is expressed as the average KL divergence between the two neighborhood distributions. By using different neighborhood definitions, we show that I-Con seamlessly incorporates methods from clustering, dimensionality reduction, and contrastive learning.</h1><img src="images/method_overview_opt.jpg" loading="lazy" sizes="100vw" srcset="images/method_overview_opt-p-500.jpg 500w, images/method_overview_opt-p-800.jpg 800w, images/method_overview_opt-p-1080.jpg 1080w, images/method_overview_opt-p-1600.jpg 1600w, images/method_overview_opt-p-2000.jpg 2000w, images/method_overview_opt-p-2600.jpg 2600w, images/method_overview_opt.jpg 2910w" alt="" class="paper-image">
      </div>
      <div class="paper-page-section-width">
        <h1 class="paper-section-header">Unifying Machine Learning Losses</h1>
        <div class="small-spacer"></div>
        <h1 class="paper-page-text-block">To see how I-Con unifies common machine learning algorithms, let&#x27;s look at a few specific examples. I-Con unifies popular methods like dimensionality reduction (SNE, t-SNE), contrastive learning (SimCLR), and clustering (k-Means), supervised learning and many more through a single mathematical equation that matches two neighborhood distributions. In methods like stochastic neighbor embedding (SNE), I-Con matches high-dimensional Gaussian neighborhoods with lower-dimensional embeddings. Replacing low-dimensional embeddings with a probability distribution over clusters results in k-Means. Switching Gaussian neighborhoods to graph edge-based neighbors yields Spectral clustering. Using data-augmentation pairs and Gaussian neighborhoods recovers the InfoNCE loss used in contrastive learning methods like CLIP and SimCLR. Beyond re-deriving existing methods, I-Con can also create new ones by mixing neighborhood definitions, such as our contrastive clustering method that combines augmentation pairs and cluster probabilities.</h1><img src="images/unifying_losses_opt.jpg" loading="lazy" sizes="100vw" srcset="images/unifying_losses_opt-p-500.jpg 500w, images/unifying_losses_opt-p-800.jpg 800w, images/unifying_losses_opt-p-1080.jpg 1080w, images/unifying_losses_opt-p-1600.jpg 1600w, images/unifying_losses_opt-p-2000.jpg 2000w, images/unifying_losses_opt.jpg 3074w" alt="" class="paper-image">
      </div>
      <div class="paper-page-section-width">
        <h1 class="paper-section-header">Deriving New Methods</h1>
        <div class="small-spacer"></div>
        <h1 class="paper-page-text-block">The I-Con framework not only unifies existing methods but also guides the discovery of new algorithms. We organize methods into a periodic table by breaking them into rows and columns based on the types of neighborhood data they use for their learned and supervisory signals. Rows correspond to different types of learned distributions (like clusters or low-dimensional embeddings), while columns correspond to different types of supervisory signals (like Gaussian neighbors or graph edges). This structured organization reveals gaps where new methods can be developed. Additionally, this shared mathematical foundation allows ideas to be transferred across algorithms, leading to innovations like debiased clustering techniques.</h1><img src="images/periodic_table.svg" loading="lazy" alt="" class="paper-image">
      </div>
      <div class="paper-page-section-width">
        <h1 class="paper-section-header">Results</h1>
        <div class="small-spacer"></div>
        <h1 class="paper-page-text-block">We compare I-Con-derived methods to state-of-the-art unsupervised image classification techniques like SCAN and TEMI using the ImageNet-1K dataset and the DINO backbone. I-Con-based image clustering consistently outperforms existing approaches. Our Debiased InfoNCE Clustering method improves over the previous art TEMI across all backbone sizes. The improvements come from I-Con’s self-balancing loss and its ability to integrate insights from contrastive learning and dimensionality reduction into clustering.</h1><img src="images/icon-table.png" loading="lazy" sizes="100vw" srcset="images/icon-table-p-500.png 500w, images/icon-table-p-800.png 800w, images/icon-table-p-1080.png 1080w, images/icon-table.png 1517w" alt="" class="paper-image">
      </div>
    </div>
  </div>
  <div id="Paper" class="section">
    <div class="section-margin">
      <div class="paper-page-section-width">
        <h1 class="paper-section-header">Paper</h1>
        <div class="small-spacer"></div>
        <a href="https://aka.ms/icon-paper" target="_blank" class="w-inline-block"><img src="images/icon_paper_img.jpg" loading="lazy" sizes="100vw" srcset="images/icon_paper_img-p-500.jpg 500w, images/icon_paper_img-p-800.jpg 800w, images/icon_paper_img-p-1080.jpg 1080w, images/icon_paper_img-p-1600.jpg 1600w, images/icon_paper_img-p-2000.jpg 2000w, images/icon_paper_img.jpg 2675w" alt="" class="paper-thumbnail"></a>
      </div>
      <div class="paper-page-section-width">
        <h1 class="paper-section-header">Bibtex</h1>
        <div class="small-spacer"></div>
        <div class="code-block">@inproceedings{<br>    alshammari2025a,<br>    title={A Unifying Framework for Representation Learning},<br>    author={Shaden Naif Alshammari and Mark Hamilton and Axel Feldmann and John R. Hershey and William T. Freeman},<br>    booktitle={The Thirteenth International Conference on Learning Representations},<br>    year={2025},<br>    url={https://openreview.net/forum?id=WfaQrKCr4X}<br>}</div>
      </div>
    </div>
  </div>
  <div id="Related-Projects" class="section">
    <div class="section-margin">
      <div class="paper-page-section-width">
        <h1 class="paper-section-header">Related Projects</h1>
        <div class="small-spacer"></div>
        <div class="paper-entry">
          <div class="pub-image-div"><img src="images/ezgif.com-gif-maker-3.gif" width="436" alt="" loading="lazy" class="pub-img"></div>
          <div class="pub-item">
            <h2 class="paper-titile">Unsupervised Semantic Segmentation by Distilling Feature Correspondences<br></h2>
            <div class="paper-buttons">
              <a target="_blank" href="stego.html" class="button w-button">Website</a>
              <a target="_blank" href="https://arxiv.org/abs/2203.08414" class="button w-button">Paper</a>
              <a target="_blank" href="https://iclr.cc/virtual/2022/poster/6068" class="button w-button">Talk</a>
              <a target="_blank" href="https://github.com/mhamilton723/STEGO" class="button w-button">Github</a>
              <a target="_blank" href="https://raw.githubusercontent.com/mhamilton723/mhamilton723.github.io/master/files/bib/hamilton2022unsupervised.bib" class="button w-button">BibTex</a>
            </div>
            <h1 class="author-list">We show that through an I-Con style loss its possible to classify every pixel from in an image dataset without any supervision, labels, or human feedback of any kind.</h1>
          </div>
        </div>
        <div class="paper-entry">
          <div class="pub-image-div">
            <div class="pub-img w-embed">
              <style>
    @media (max-width: 480px) {
        .responsive-video {
            max-height: 200px;
        }
    }
</style>
              <video playsinline="" autoplay="" loop="" preload="" muted="" class="responsive-video" style="max-width: 100%; object-fit: contain;">
                <source src="https://mhamilton.net/videos/denseav_teaser_trimmed_silent.mp4" type="video/mp4">
              </video>
            </div>
          </div>
          <div class="pub-item">
            <h2 class="paper-titile">Separating the &quot;Chirp&quot; from the &quot;Chat&quot;: Self-supervised Visual Grounding of Sound and Language<br></h2>
            <div class="paper-buttons">
              <a target="_blank" href="denseav.html" class="button w-button">Website</a>
              <a target="_blank" href="https://aka.ms/denseav-paper" class="button w-button">Paper</a>
              <a target="_blank" href="https://www.youtube.com/watch?v=rD2kfwu1fYE&amp;list=PLBJWRPcgwk7vVzKLPnTrqm831VohoLMmy&amp;index=4" class="button w-button">Talk</a>
              <a target="_blank" href="https://github.com/mhamilton723/DenseAV" class="button w-button">Github</a>
              <a target="_blank" href="https://raw.githubusercontent.com/mhamilton723/mhamilton723.github.io/master/files/bib/hamilton2024separating.bib" class="button w-button">BibTex</a>
            </div>
            <h1 class="author-list">We show that through an I-Con style loss its possible to rediscover the meaning of language just by watching videos. No text or supervision is used in the algorithm. </h1>
          </div>
        </div>
      </div>
    </div>
  </div>
  <div id="Related-Projects" class="section">
    <div id="Contact" class="section-margin">
      <div class="paper-page-section-width">
        <h1 class="paper-section-header">Contact</h1>
        <div class="small-spacer"></div>
        <h1 class="paper-page-text-block">For feedback, questions, or press inquiries please contact <a href="mailto:shaden@mit.edu" class="link">Shaden Alshammari</a> or <a href="mailto:markth@mit.edu" class="link">Mark Hamilton</a>
        </h1>
      </div>
      <div class="footer"></div>
    </div>
  </div>
  <script src="https://d3e54v103j8qbb.cloudfront.net/js/jquery-3.5.1.min.dc5e7f18c8.js?site=5fd97fcae7aaf2c02444f619" type="text/javascript" integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin="anonymous"></script>
  <script src="js/webflow.js" type="text/javascript"></script><!-- <script async src="lib/particles.min.js"></script>  -->
</body>
</html>