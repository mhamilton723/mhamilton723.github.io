<!DOCTYPE html><!--  This site was created in Webflow. https://webflow.com  --><!--  Last Published: Mon Nov 18 2024 04:41:03 GMT+0000 (Coordinated Universal Time)  -->
<html data-wf-page="644c34eaf49920e8354bf7ea" data-wf-site="5fd97fcae7aaf2c02444f619">
<head>
  <meta charset="utf-8">
  <title>Exploring Perceptual Straightening In Learned Visual Representations</title>
  <meta content="Project page for the ICLR 2023 paper: Exploring Perceptual Straightening In Learned Visual Representations" name="description">
  <meta content="Exploring Perceptual Straightening In Learned Visual Representations" property="og:title">
  <meta content="Project page for the ICLR 2023 paper: Exploring Perceptual Straightening In Learned Visual Representations" property="og:description">
  <meta content="https://mhamilton.net/images/perceptual_straightening_teaser.jpg" property="og:image">
  <meta content="Exploring Perceptual Straightening In Learned Visual Representations" property="twitter:title">
  <meta content="Project page for the ICLR 2023 paper: Exploring Perceptual Straightening In Learned Visual Representations" property="twitter:description">
  <meta content="https://mhamilton.net/images/perceptual_straightening_teaser.jpg" property="twitter:image">
  <meta property="og:type" content="website">
  <meta content="summary_large_image" name="twitter:card">
  <meta content="width=device-width, initial-scale=1" name="viewport">
  <meta content="Webflow" name="generator">
  <link href="css/normalize.css" rel="stylesheet" type="text/css">
  <link href="css/webflow.css" rel="stylesheet" type="text/css">
  <link href="css/mhamilton723.webflow.css" rel="stylesheet" type="text/css">
  <link href="https://fonts.googleapis.com" rel="preconnect">
  <link href="https://fonts.gstatic.com" rel="preconnect" crossorigin="anonymous">
  <script src="https://ajax.googleapis.com/ajax/libs/webfont/1.6.26/webfont.js" type="text/javascript"></script>
  <script type="text/javascript">WebFont.load({  google: {    families: ["Inconsolata:400,700","Roboto:100,300,regular,500,700"]  }});</script>
  <script type="text/javascript">!function(o,c){var n=c.documentElement,t=" w-mod-";n.className+=t+"js",("ontouchstart"in o||o.DocumentTouch&&c instanceof DocumentTouch)&&(n.className+=t+"touch")}(window,document);</script>
  <link href="images/favicon.ico" rel="shortcut icon" type="image/x-icon">
  <link href="images/webclip.png" rel="apple-touch-icon">
  <script async="" src="https://www.googletagmanager.com/gtag/js?id=G-V1K6GK8BBM"></script>
  <script type="text/javascript">window.dataLayer = window.dataLayer || [];function gtag(){dataLayer.push(arguments);}gtag('js', new Date());gtag('set', 'developer_id.dZGVlNj', true);gtag('config', 'G-V1K6GK8BBM');</script>
</head>
<body class="body">
  <div data-collapse="small" data-animation="over-right" data-duration="400" data-easing="ease" data-easing2="ease" role="banner" class="nav w-nav">
    <div class="nav-container w-container">
      <nav role="navigation" class="nav-menu w-nav-menu">
        <div class="div-block">
          <a href="#Video" class="nav-link w-nav-link">Video</a>
          <a href="#Abstract" class="nav-link w-nav-link">Abstract</a>
          <a href="#About" class="nav-link w-nav-link">About</a>
          <a href="#Paper" class="nav-link w-nav-link">Paper</a>
          <a href="#Contact" class="nav-link w-nav-link">Contact</a>
        </div>
      </nav>
      <div class="menu-button w-nav-button">
        <div class="icon w-icon-nav-menu"></div>
      </div>
    </div>
  </div>
  <div id="Hero" class="paper-hero">
    <div class="hero-wrap"></div>
    <h1 class="paper-title">Exploring Perceptual Straightness in<br>Learned Visual Representations</h1>
    <h1 class="h2">ICLR 2023</h1>
    <div class="paper-button-holder">
      <a target="_blank" href="https://openreview.net/pdf?id=4cOfD2qL6T" class="button w-button">Paper</a>
      <a target="_blank" href="https://www.youtube.com/watch?v=cXjJeARZhD4" class="button w-button">Talk</a>
    </div>
    <div class="spacer"></div>
    <h1 class="paper-names">
      <a href="https://scholar.google.com/citations?user=7M9eSFMAAAAJ&amp;hl=en" target="_blank" class="link">Anne Harrington</a>, <a href="https://vashadutell.com/" target="_blank" class="link">Vasha DuTell</a>, <a href="https://ayushtewari.com/" target="_blank" class="link">Ayush Tewari</a>, <a href="https://mhamilton.net" target="_blank" class="link">Mark Hamilton</a>, <a href="https://scholar.google.com/citations?user=f3aij5UAAAAJ&amp;hl=en" target="_blank" class="link">Simon Stent</a>, <a href="http://persci.mit.edu/people/rosenholtz" target="_blank" class="link">Ruth Rosenholtz</a>, <a href="https://billf.mit.edu/about/bio" target="_blank" class="link">William T. Freeman</a>
    </h1>
    <div class="w-layout-grid affiliationholder"><img src="images/CSAIL_Primary_Regular_Monochrome.svg" loading="lazy" alt="" id="w-node-_185912c3-010d-fb94-8b11-ceb325f173f4-354bf7ea" class="affiliation-image"><img src="images/mit_logo_white.svg" loading="lazy" alt="" class="affiliation-image"><img src="images/Woven_Planet_logomark_2021.svg" loading="lazy" alt="" class="affiliation-image"></div>
  </div>
  <div id="Video" class="section">
    <div class="section-margin">
      <div class="paper-page-section-width">
        <div style="padding-top:56.17021276595745%" class="w-embed-youtubevideo youtube"><iframe src="https://www.youtube.com/embed/cXjJeARZhD4?rel=0&amp;controls=1&amp;autoplay=0&amp;mute=0&amp;start=0" frameborder="0" style="position:absolute;left:0;top:0;width:100%;height:100%;pointer-events:auto" allow="autoplay; encrypted-media" allowfullscreen="" title="ICLR 2023: Exploring Perceptual Straightening In Learned Visual Representations"></iframe></div>
      </div>
    </div>
  </div>
  <div id="Abstract" class="section">
    <div class="section-margin">
      <div class="paper-page-section-width">
        <div class="small-spacer"></div>
        <h1 class="paper-page-text-block">Humans have been shown to use a “straightened” encoding to represent the natural visual world as it evolves in time (Henaff et al. 2019). In the context of ´discrete video sequences, “straightened” means that changes between frames follow a more linear path in representation space at progressively deeper levels of processing. While deep convolutional networks are often proposed as models of human visual processing, many do not straighten natural videos. In this paper, we explore the relationship between network architecture, differing types of robustness, biologically-inspired filtering mechanisms, and representational straightness in response to time-varying input; we identify strengths and limitations of straightness as a useful way of evaluating neural network representations. We find that (1) adversarial training leads to straighter representations in both CNN and transformer-based architectures but (2) this effect is task-dependent, not generalizing to tasks such as segmentation and frame-prediction, where straight representations are not favorable for predictions; and nor to other types of robustness. In addition, (3) straighter representations impart temporal stability to class predictions, even for out-of-distribution data. Finally, (4) biologically-inspired elements increase straightness in the early stages of a network, but do not guarantee increased straightness in downstream layers of CNNs. We show that straightness is an easily computed measure of representational robustness and stability, as well as a hallmark of human representations with benefits for computer vision models.</h1>
      </div>
    </div>
  </div>
  <div id="About" class="section">
    <div class="section-margin">
      <div class="paper-page-section-width">
        <h1 class="paper-section-header">The Straightness of a Representation</h1>
        <div class="small-spacer"></div>
        <h1 class="paper-page-text-block">The curvature of a representation refers to the smoothness or linearity of the trajectory of visual input over time in some representation space. <strong>Humans tend to use a straightened encoding to represent the natural visual world</strong> as it evolves in time (<a href="#" class="link">Henaff et.al.</a>).  Straighter representations can be useful for visual tasks that require extrapolation, such as predicting the future visual state of the world. We investigate how and where representational straigntness arises in algorithms.</h1><img src="images/percpetual_straightening_teaser_white.svg" loading="lazy" alt="" class="paper-image">
      </div>
      <div class="paper-page-section-width">
        <h1 class="paper-section-header">Measuring Straightness</h1>
        <div class="small-spacer"></div>
        <h1 class="paper-page-text-block">To compute the curvature of a video sequence, we look at the velocity vectors of the representation over time. This can be done at any stage of the processing pipeline, from raw input pixels to activations of a network&#x27;s hidden layer. To compute the curvature at each time step <strong>we find the angle between successive velocity vectors</strong>. We define the <strong>global curvature as the average angle over all time</strong> steps. This provides a simple measurement of how straight the sequence of video frame representations is.</h1><img src="images/cossimvcurvature_white.png" loading="lazy" sizes="(max-width: 479px) 93vw, (max-width: 767px) 95vw, (max-width: 991px) 90vw, 749.9999389648438px" srcset="images/cossimvcurvature_white-p-500.png 500w, images/cossimvcurvature_white-p-800.png 800w, images/cossimvcurvature_white-p-1080.png 1080w, images/cossimvcurvature_white-p-1600.png 1600w, images/cossimvcurvature_white.png 1703w" alt="" class="paper-image">
      </div>
      <div class="paper-page-section-width">
        <h1 class="paper-section-header">Robust Models have Straighter Representations</h1>
        <div class="small-spacer"></div>
        <h1 class="paper-page-text-block">We explored the relationship between model type, adversarial attack type and strength, and output curvature in image recognition models. We find that <strong>adversarially trained models tend to have lower curvature</strong> compared to non-adversarially trained models. Furthermore, we show that higher robustness to adversarial perturbations leads to straighter representations. This is particularly useful in fields such as robotic vision where objects are observed from different angles over time. Overall, we found that <strong>lower curvature results in a more stable predictions</strong> over time, which is beneficial in various applications.</h1><img src="images/bar_demioutput_cropped_bigFont_black.png" loading="lazy" sizes="(max-width: 479px) 93vw, (max-width: 767px) 95vw, (max-width: 991px) 90vw, 749.9999389648438px" srcset="images/bar_demioutput_cropped_bigFont_black-p-500.png 500w, images/bar_demioutput_cropped_bigFont_black-p-800.png 800w, images/bar_demioutput_cropped_bigFont_black-p-1080.png 1080w, images/bar_demioutput_cropped_bigFont_black-p-1600.png 1600w, images/bar_demioutput_cropped_bigFont_black-p-2000.png 2000w, images/bar_demioutput_cropped_bigFont_black-p-2600.png 2600w, images/bar_demioutput_cropped_bigFont_black.png 3000w" alt="" class="paper-image">
      </div>
    </div>
  </div>
  <div id="Paper" class="section">
    <div class="section-margin">
      <div class="paper-page-section-width">
        <h1 class="paper-section-header">Paper</h1>
        <div class="small-spacer"></div>
        <a href="https://openreview.net/pdf?id=4cOfD2qL6T" target="_blank" class="w-inline-block"><img src="images/paper_teaser.jpg" loading="lazy" sizes="(max-width: 479px) 93vw, (max-width: 767px) 95vw, (max-width: 991px) 90vw, 749.9999389648438px" srcset="images/paper_teaser-p-500.jpg 500w, images/paper_teaser-p-800.jpg 800w, images/paper_teaser-p-1080.jpg 1080w, images/paper_teaser-p-1600.jpg 1600w, images/paper_teaser.jpg 2000w" alt="" class="paper-thumbnail"></a>
      </div>
      <div class="paper-page-section-width">
        <h1 class="paper-section-header">Bibtex</h1>
        <div class="small-spacer"></div>
        <div class="code-block">@inproceedings{<br>    harringtonexploring,<br>    title={Exploring perceptual straightness in learned visual representations},<br>    author={<br>        Harrington, Anne and<br>        DuTell, Vasha and<br>        Tewari, Ayush and<br>        Hamilton, Mark and<br>        Stent, Simon and<br>        Rosenholtz, Ruth and<br>        Freeman, William T},<br>    booktitle={The Eleventh International Conference on Learning Representations}<br>}<br></div>
      </div>
    </div>
  </div>
  <div id="Related-Projects" class="section">
    <div class="section-margin"></div>
  </div>
  <div id="Related-Projects" class="section">
    <div id="Contact" class="section-margin">
      <div class="paper-page-section-width">
        <h1 class="paper-section-header">Contact</h1>
        <div class="small-spacer"></div>
        <h1 class="paper-page-text-block">For feedback, questions, or press inquiries please contact annekh@mit.edu and vasha@mit.edu</h1>
      </div>
      <div class="footer"></div>
    </div>
  </div>
  <script src="https://d3e54v103j8qbb.cloudfront.net/js/jquery-3.5.1.min.dc5e7f18c8.js?site=5fd97fcae7aaf2c02444f619" type="text/javascript" integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin="anonymous"></script>
  <script src="js/webflow.js" type="text/javascript"></script><!-- <script async src="lib/particles.min.js"></script>  -->
</body>
</html>